{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5eb96d-fd04-4697-a73d-2251e4621bdc",
   "metadata": {},
   "source": [
    "Training just the EWT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2611f1ed-604c-4f4f-872f-e3c227b85fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogu/.conda/envs/bert-try/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100-PCIE-40GB\n",
      "  Memory Allocated: 0.00 GB\n",
      "  Memory Cached: 0.00 GB\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "# Data Reading and Preprocessing Functions\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "    \n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./en_ewt-ud-train.iob2\")\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "def replace_per_tags(tags_list):\n",
    "    updated_tags_list = []\n",
    "    for tags in tags_list:\n",
    "        updated_tags = []\n",
    "        for tag in tags:\n",
    "            if tag == \"B-PER\":\n",
    "                updated_tags.append(\"B-CHAR\")\n",
    "            elif tag == \"I-PER\":\n",
    "                updated_tags.append(\"I-CHAR\")\n",
    "            else:\n",
    "                updated_tags.append(tag)\n",
    "        updated_tags_list.append(updated_tags)\n",
    "    return updated_tags_list\n",
    "\n",
    "train_tags = replace_per_tags(train_tags)\n",
    "\n",
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cec580-a99f-4d9c-ad24-de07081d04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a1ad31-c0d3-44a8-8d3c-8fdb44768785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_counts_train: \n",
      "O: 194219\n",
      "B-LOC: 2712\n",
      "I-LOC: 877\n",
      "B-CHAR: 2874\n",
      "B-ORG: 1436\n",
      "I-ORG: 1167\n",
      "I-CHAR: 1294\n",
      "labels_to_ids: {'B-ORG': 0, 'B-LOC': 1, 'I-ORG': 2, 'I-LOC': 3, 'B-CHAR': 4, 'O': 5, 'I-CHAR': 6}\n",
      "ids_to_labels: {0: 'B-ORG', 1: 'B-LOC', 2: 'I-ORG', 3: 'I-LOC', 4: 'B-CHAR', 5: 'O', 6: 'I-CHAR'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts_train = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in train_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts_train[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts_train = dict(tag_counts_train)\n",
    "\n",
    "# Print the counts for each tag\n",
    "print('tag_counts_train: ')\n",
    "for tag, count in tag_counts_train.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "# Create mappings\n",
    "all_tags = [tag for tags in df['word_labels'] for tag in tags.split(\",\")]\n",
    "unique_tags = set(all_tags)\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_tags)}\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "\n",
    "# Display the mappings\n",
    "print(\"labels_to_ids:\", labels_to_ids)\n",
    "print(\"ids_to_labels:\", ids_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62797808-8374-4af6-8a5f-180404114fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tag counts in train_tags: {'O': 194219, 'B-LOC': 2712, 'I-LOC': 877, 'B-CHAR': 2874, 'B-ORG': 1436, 'I-ORG': 1167, 'I-CHAR': 1294}\n",
      "Initial tag counts in test_tags: {'O': 25000, 'B-CHAR': 820, 'I-CHAR': 85, 'B-LOC': 216, 'B-ORG': 2, 'I-LOC': 2}\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing datasets\n",
    "training_set = dataset(df, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "initial_train_tag_counts = count_tags(train_tags)\n",
    "print(\"Initial tag counts in train_tags:\", dict(initial_train_tag_counts))\n",
    "\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14acec1-81a0-40c6-8b86-4e24e05892f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels_to_ids))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c249e27-5567-4497-a2ee-00cb4775ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df.to_csv('data/EWT-training_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f976551-4451-49ac-b118-f86587b80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports_df.to_csv('data/EWT-training-report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51ada9-42d3-43a5-ba0f-964a9d6133c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"EWT-baseline\")\n",
    "# tokenizer.save_pretrained('EWT-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014a36b-d000-4b3f-909c-379ef392f274",
   "metadata": {},
   "outputs": [],
   "source": [
    " # import json\n",
    "# config = json.load(open('EWT-baseline/config.json'))\n",
    "# config['id2label'] = ids_to_labels\n",
    "# config['label2id'] = labels_to_ids\n",
    "# json.dump(config, open('EWT-baseline/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c5ca0-45dc-4723-82b6-453b2b26c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "# Data Reading and Preprocessing Functions\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "\n",
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in test_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts = dict(tag_counts)\n",
    "\n",
    "# Print the counts for each tag\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "labels_to_ids = {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
    "ids_to_labels = {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
    "\n",
    "# Create training and testing datasets\n",
    "training_set = dataset(df, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a574c-b2db-47be-beba-cc6416bbf681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595eb4f-5ecd-4686-ac71-095df608aec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cfba2-a973-45fb-96cc-9c2ced42cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df.to_csv('data/EWT_LOTR_metrics.csv', index=False)\n",
    "# reports_df.to_csv('data/EWT_LOTR_reports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9520ae-a7e5-4f5f-b8f1-d4f241c4019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"EWT-LOTR-baseline\")\n",
    "# tokenizer.save_pretrained('EWT-LOTR-tokenizer')\n",
    "# import json\n",
    "# config = json.load(open('EWT-LOTR-baseline/config.json'))\n",
    "# config['id2label'] = ids_to_labels\n",
    "# config['label2id'] = labels_to_ids\n",
    "# json.dump(config, open('EWT-LOTR-baseline/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dc5fd-603c-46c8-afdb-69f9ab4591e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f8312-e5d9-4e69-b4a0-89786f5e1d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cbe3a-a407-4d58-9389-267892036ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03266b-51eb-4943-be34-8b106cb9ead7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75750a02-a96c-41f3-8aa5-7fdc82f2d481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53666b5d-0f1c-4121-b04b-891adf6b31ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924b241-c179-4ad1-9ead-2cc4a3db0785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-try)",
   "language": "python",
   "name": "bert-try"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
