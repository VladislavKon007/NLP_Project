{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12741e52-d969-46b5-8a8d-bf8972c49bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53f7e30-552c-4123-be43-9cbc910725d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MODEL_PATH = 'ner_model_from_final1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cb109a-3818-4907-a392-9023b26d38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tag(tag):\n",
    "    # Ensure tags are in the correct format\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2133afc-6c1f-4943-8330-4d4e708b8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_names(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        names = [name.strip().lower() for name in file.readlines()]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efc9e0-51ae-42cb-ab53-0115685d9dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f239e5-a962-422a-98dc-e23c035931e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, tag2idx, idx2tag):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "            # Collect the predictions and true labels for calculating F1 score and accuracy\n",
    "            all_preds.extend(predictions.cpu().numpy().tolist())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Flatten the lists to calculate metrics\n",
    "    all_preds_flat = [p for preds in all_preds for p in preds]\n",
    "    all_labels_flat = [l for labels in all_labels for l in labels]\n",
    "\n",
    "    # Remove padding tokens, the label 0 (O), and -100 for accuracy and F1 calculation\n",
    "    true_preds = [pred for pred, label in zip(all_preds_flat, all_labels_flat) if label != tag2idx['PAD'] and label != tag2idx['O'] and label != -100]\n",
    "    true_labels = [label for label in all_labels_flat if label != tag2idx['PAD'] and label != tag2idx['O'] and label != -100]\n",
    "\n",
    "    # Map indices back to tags\n",
    "    true_preds_tags = [idx2tag[pred] for pred in true_preds]\n",
    "    true_labels_tags = [idx2tag[label] for label in true_labels]\n",
    "\n",
    "    # Get the list of unique tags in the dataset (excluding PAD and O)\n",
    "    unique_tags = [tag for tag in tag2idx if tag != 'PAD' and tag != 'O']\n",
    "\n",
    "    f1 = f1_score(true_labels_tags, true_preds_tags, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels_tags, true_preds_tags)\n",
    "\n",
    "    print(f'Average Loss: {avg_loss}')\n",
    "    print(f'F1 Score (excluding PAD and O): {f1}')\n",
    "    print(f'Accuracy (excluding PAD and O): {accuracy}')\n",
    "    print(classification_report(true_labels_tags, true_preds_tags, labels=unique_tags, target_names=unique_tags))\n",
    "\n",
    "    return avg_loss, f1, accuracy, true_labels_tags, true_preds_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25ea81d-ce15-4639-9c23-f850bd1a5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2140f4d-cced-4cb2-a4f9-3be204cf7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tokenizer, max_len, tag2idx):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2idx = tag2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        word_labels = self.tags[idx]\n",
    "        encoding = self.tokenizer(sentence, is_split_into_words=True, return_offsets_mapping=True, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        labels = [self.tag2idx['O']] * self.max_len  # Initialize labels with \"O\"\n",
    "        offsets = encoding['offset_mapping'].squeeze().tolist()  # Get the offsets\n",
    "        encoding.pop('offset_mapping')  # Remove offsets, not needed for model input\n",
    "\n",
    "        idx = 0\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if start == end:  # Special tokens\n",
    "                labels[i] = self.tag2idx['O']\n",
    "            elif start == 0:  # Start of a new word\n",
    "                if idx < len(word_labels):\n",
    "                    labels[i] = self.tag2idx[word_labels[idx]]\n",
    "                else:\n",
    "                    labels[i] = self.tag2idx['O']\n",
    "                idx += 1\n",
    "            else:  # Subtoken of a word\n",
    "                labels[i] = -100  # PyTorch's convention to ignore these tokens in loss computation\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}  # Remove batch dimension\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1392e95c-2766-4d2b-a6c3-6354385e7061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New NERDataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tokenizer, max_len, tag2idx):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2idx = tag2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        word_labels = self.tags[idx]\n",
    "        encoding = self.tokenizer(sentence, is_split_into_words=True, return_offsets_mapping=True, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        labels = [self.tag2idx['O']] * self.max_len  # Initialize labels with \"O\"\n",
    "        offsets = encoding['offset_mapping'].squeeze().tolist()  # Get the offsets\n",
    "        encoding.pop('offset_mapping')  # Remove offsets, not needed for model input\n",
    "\n",
    "        idx = 0\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if start == end:  # Special tokens\n",
    "                labels[i] = self.tag2idx['O']\n",
    "            elif start == 0:  # Start of a new word\n",
    "                if idx < len(word_labels):\n",
    "                    labels[i] = self.tag2idx[word_labels[idx]]\n",
    "                else:\n",
    "                    labels[i] = self.tag2idx['O']\n",
    "                idx += 1\n",
    "            else:  # Subtoken of a word\n",
    "                labels[i] = -100  # PyTorch's convention to ignore these tokens in loss computation\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}  # Remove batch dimension\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ccbc2a-1ae1-456a-9e0d-17e768861405",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_names = read_names('./scraping_res/character_names.txt')\n",
    "location_names = read_names('./scraping_res/location_names.txt')\n",
    "organization_names = read_names('./scraping_res/organization_names.txt')\n",
    "all_names = character_names + location_names + organization_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f7498d-c408-4a04-883d-b7e9b661e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogu/.conda/envs/bert-try/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "num_added_toks = tokenizer.add_tokens(all_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a72edd4-14a5-4790-adb8-df423425d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ed45ad-382d-40a9-992f-bebe38fdba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = list(set(tag for doc in train_tags for tag in doc))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tag_values)}\n",
    "idx2tag = dict([(value, key) for key, value in tag2idx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc2f199d-8160-47f9-a3cc-8f6553e185e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NERDataset(train_tokens, train_tags, tokenizer, MAX_LEN, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2c426b-8faa-42ee-b90c-74ad5db15e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2023,  2338,  2003,  1048, 12098, 21500,  2100,  4986,  2007,\n",
       "         31038,  1055,  1998,  2013,  2049,  5530,  1037,  1054, 31581,  4315,\n",
       "          2089, 30702,  3104,  2172,  1997,  2037, 10381, 12098,  2552,  2121,\n",
       "          1998,  1037,  2210,  1997,  2037,  2381,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([   3,    3,    3,    3,    3, -100, -100, -100,    3,    3,    2, -100,\n",
       "            3,    3,    3,    3,    3,    3, -100, -100,    3,    3, -100,    3,\n",
       "            3,    3,    3, -100, -100, -100,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc8d8f86-eb03-4f7d-a5c1-03061676ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_tokenization(item, tokenizer, idx2tag):\n",
    "    input_ids = item['input_ids']\n",
    "    attention_mask = item['attention_mask']\n",
    "    labels = item['labels']\n",
    "\n",
    "    # Use attention_mask to filter out padding tokens\n",
    "    filtered_ids = input_ids[attention_mask == 1]\n",
    "    filtered_labels = labels[attention_mask == 1]\n",
    "\n",
    "    # Decode the filtered ids\n",
    "    decoded_sentence = tokenizer.decode(filtered_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Decode labels\n",
    "    decoded_labels = [idx2tag[label.item()] for label in filtered_labels if label != -100]\n",
    "\n",
    "    return decoded_sentence, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a34dfa87-0dcf-4583-96ba-f44a615f1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_data[0]\n",
    "reverted_sentence, reverted_labels = revert_tokenization(item, tokenizer, idx2tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a67216d-e47f-4551-a78c-61c92ef11076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "this book is largely concerned with hobbits and from its pages a reader may discover much of their character and a little of their history\n",
      "\n",
      "Reverted Sentence:\n",
      "this book is l ar gely concerned with hobbit s and from its pages a r eä der may dís cover much of their ch ar acter and a little of their history\n",
      "\n",
      "Reverted Labels:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence:\")\n",
    "print(\" \".join(train_tokens[0]))\n",
    "print(\"\\nReverted Sentence:\")\n",
    "print(reverted_sentence)\n",
    "print(\"\\nReverted Labels:\")\n",
    "print(reverted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5cbd77c-0c4a-4a32-ab70-d306cd5517fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this book is l ar gely concerned with hobbit s and from its pages a r eä der may dís cover much of their ch ar acter and a little of their history'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ef88546-faa7-49d5-8b69-6a8e3876e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: 26\n",
      "further information will also be found in the selection from the red book of westmarch that has already been published under the title of the hobbit\n",
      "Original Labels 26\n",
      "O O O O O O O O O O O O O O B-LOC O O O O O O O O O O B-CHAR\n",
      "\n",
      "Reverted Sentence: 148\n",
      "further information will also be found in the selection from the red book of westmarch that has already been published under the title of the hobbit\n",
      "\n",
      "Reverted Labels: 28\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O']\n"
     ]
    }
   ],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tokenizer, max_len, tag2idx):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2idx = tag2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        word_labels = self.tags[idx]\n",
    "        encoding = self.tokenizer(sentence, is_split_into_words=True, return_offsets_mapping=True, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        labels = [self.tag2idx['O']] * self.max_len  # Initialize labels with \"O\"\n",
    "        offsets = encoding['offset_mapping'].squeeze().tolist()  # Get the offsets\n",
    "        encoding.pop('offset_mapping')  # Remove offsets, not needed for model input\n",
    "\n",
    "        idx = 0\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if start == end:  # Special tokens\n",
    "                labels[i] = self.tag2idx['O']\n",
    "            elif start == 0:  # Start of a new word\n",
    "                if idx < len(word_labels):\n",
    "                    labels[i] = self.tag2idx[word_labels[idx]]\n",
    "                else:\n",
    "                    labels[i] = self.tag2idx['O']\n",
    "                idx += 1\n",
    "            else:  # Subtoken of a word\n",
    "                labels[i] = -100  # PyTorch's convention to ignore these tokens in loss computation\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}  # Remove batch dimension\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "def revert_tokenization(item, tokenizer, idx2tag):\n",
    "    input_ids = item['input_ids']\n",
    "    attention_mask = item['attention_mask']\n",
    "    labels = item['labels']\n",
    "\n",
    "    # Use attention_mask to filter out padding tokens\n",
    "    filtered_ids = input_ids[attention_mask == 1]\n",
    "    filtered_labels = labels[attention_mask == 1]\n",
    "\n",
    "    # Decode the filtered ids\n",
    "    decoded_sentence = tokenizer.decode(filtered_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Decode labels\n",
    "    decoded_labels = [idx2tag[label.item()] for label in filtered_labels if label != -100]\n",
    "\n",
    "    return decoded_sentence, decoded_labels\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "tag_values = list(set(tag for doc in train_tags for tag in doc))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tag_values)}\n",
    "idx2tag = dict([(value, key) for key, value in tag2idx.items()])\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_data = NERDataset(train_tokens, train_tags, tokenizer, MAX_LEN, tag2idx)\n",
    "number = 1\n",
    "# Check the \"numeer\" item in the dataset\n",
    "item = train_data[number]\n",
    "reverted_sentence, reverted_labels = revert_tokenization(item, tokenizer, idx2tag)\n",
    "\n",
    "print(\"Original Sentence:\", len(train_tokens[number]))\n",
    "print(\" \".join(train_tokens[number]))\n",
    "print('Original Labels', len(train_tags[number]))\n",
    "print(\" \".join(train_tags[number]))\n",
    "print(\"\\nReverted Sentence:\", len(reverted_sentence))\n",
    "print(reverted_sentence)\n",
    "print(\"\\nReverted Labels:\", len(reverted_labels))\n",
    "print(reverted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df13548-d863-4ce1-953d-25a0f9626c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: O\n",
      "further: O\n",
      "information: O\n",
      "will: O\n",
      "also: O\n",
      "be: O\n",
      "found: O\n",
      "in: O\n",
      "the: O\n",
      "selection: O\n",
      "from: O\n",
      "the: O\n",
      "red: O\n",
      "book: O\n",
      "of: O\n",
      "west: B-LOC\n",
      "##mar: O\n",
      "##ch: O\n",
      "that: O\n",
      "has: O\n",
      "already: O\n",
      "been: O\n",
      "published: O\n",
      "under: O\n",
      "the: O\n",
      "title: O\n",
      "of: B-CHAR\n",
      "the: O\n"
     ]
    }
   ],
   "source": [
    "def print_decoded_words_with_tags(item, tokenizer, idx2tag):\n",
    "    input_ids = item['input_ids']\n",
    "    attention_mask = item['attention_mask']\n",
    "    labels = item['labels']\n",
    "\n",
    "    # Use attention_mask to filter out padding tokens\n",
    "    filtered_ids = input_ids[attention_mask == 1]\n",
    "    filtered_labels = labels[attention_mask == 1]\n",
    "\n",
    "    # Decode the filtered ids\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(filtered_ids)\n",
    "\n",
    "    # Map label indices back to tag names\n",
    "    decoded_labels = [idx2tag[label.item()] for label in filtered_labels if label != -100]\n",
    "\n",
    "    for token, label in zip(decoded_tokens, decoded_labels):\n",
    "        print(f\"{token}: {label}\")\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "tag_values = list(set(tag for doc in train_tags for tag in doc))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tag_values)}\n",
    "idx2tag = dict([(value, key) for key, value in tag2idx.items()])\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_data = NERDataset(train_tokens, train_tags, tokenizer, MAX_LEN, tag2idx)\n",
    "\n",
    "# Check the first item in the dataset\n",
    "item = train_data[1]\n",
    "\n",
    "# Print each word with its tag\n",
    "print_decoded_words_with_tags(item, tokenizer, idx2tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb4ffc-de96-4e18-9381-848534d76f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e64e0-c628-451c-a732-b7421a8b4281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "579da249-6d31-4d65-a33e-efb1285f6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be472417-71c4-4c88-9952-ed79ba2f43f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids tensor: 128\n",
      "Length of attention_mask tensor: 128\n"
     ]
    }
   ],
   "source": [
    "# Assuming item is the dictionary containing 'input_ids' and 'attention_mask'\n",
    "input_ids = item['input_ids']\n",
    "attention_mask = item['attention_mask']\n",
    "\n",
    "# Get the length of the input_ids tensor\n",
    "length_input_ids = input_ids.size(0)\n",
    "\n",
    "# Get the length of the attention_mask tensor\n",
    "length_attention_mask = attention_mask.size(0)\n",
    "\n",
    "print(\"Length of input_ids tensor:\", length_input_ids)\n",
    "print(\"Length of attention_mask tensor:\", length_attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e53d5704-5a22-40aa-a3de-78072ee93ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lengths(item):\n",
    "    input_ids = item['input_ids']\n",
    "    attention_mask = item['attention_mask']\n",
    "    labels = item['labels']\n",
    "\n",
    "    # Apply the attention mask to filter out padding tokens\n",
    "    filtered_input_ids = input_ids[attention_mask == 1]\n",
    "    filtered_labels = labels[attention_mask == 1]\n",
    "\n",
    "    # Get the lengths after applying the attention mask\n",
    "    length_input_ids = filtered_input_ids.size(0)\n",
    "    length_labels = filtered_labels.size(0)\n",
    "\n",
    "    print(\"Length of input_ids tensor after applying attention mask:\", length_input_ids)\n",
    "    print(\"Length of labels tensor after applying attention mask:\", length_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc9f1c75-ca49-4fd6-b385-47230943eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids tensor after applying attention mask: 29\n",
      "Length of labels tensor after applying attention mask: 29\n"
     ]
    }
   ],
   "source": [
    "print_lengths(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efde9968-cdf1-42bc-bf35-18c4049907d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: O\n",
      "further: O\n",
      "information: O\n",
      "will: O\n",
      "also: O\n",
      "be: O\n",
      "found: O\n",
      "in: O\n",
      "the: O\n",
      "selection: O\n",
      "from: O\n",
      "the: O\n",
      "red: O\n",
      "book: O\n",
      "of: O\n",
      "west: B-LOC\n",
      "##mar: O\n",
      "##ch: O\n",
      "that: O\n",
      "has: O\n",
      "already: O\n",
      "been: O\n",
      "published: O\n",
      "under: O\n",
      "the: O\n",
      "title: O\n",
      "of: B-CHAR\n",
      "the: I-CHAR\n",
      "ho: O\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tokenizer, max_len, tag2idx):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2idx = tag2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        word_labels = self.tags[idx]\n",
    "        encoding = self.tokenizer(sentence, is_split_into_words=True, return_offsets_mapping=True, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        labels = [self.tag2idx['O']] * self.max_len  # Initialize labels with \"O\"\n",
    "        offsets = encoding['offset_mapping'].squeeze().tolist()  # Get the offsets\n",
    "        encoding.pop('offset_mapping')  # Remove offsets, not needed for model input\n",
    "\n",
    "        idx = 0\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if start == end:  # Special tokens\n",
    "                labels[i] = self.tag2idx['O']\n",
    "            elif start == 0:  # Start of a new word\n",
    "                if idx < len(word_labels):\n",
    "                    labels[i] = self.tag2idx[word_labels[idx]]\n",
    "                else:\n",
    "                    labels[i] = self.tag2idx['O']\n",
    "                idx += 1\n",
    "            else:  # Subtoken of a word\n",
    "                if labels[i - 1] == self.tag2idx['B-CHAR']:\n",
    "                    labels[i] = self.tag2idx.get('I-CHAR', -100)  # Assign I-CHAR if exists\n",
    "                else:\n",
    "                    labels[i] = -100  # PyTorch's convention to ignore these tokens in loss computation\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}  # Remove batch dimension\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "def print_decoded_words_with_tags(item, tokenizer, idx2tag):\n",
    "    input_ids = item['input_ids']\n",
    "    attention_mask = item['attention_mask']\n",
    "    labels = item['labels']\n",
    "\n",
    "    # Use attention_mask to filter out padding tokens\n",
    "    filtered_ids = input_ids[attention_mask == 1]\n",
    "    filtered_labels = labels[attention_mask == 1]\n",
    "\n",
    "    # Decode the filtered ids\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(filtered_ids)\n",
    "\n",
    "    # Map label indices back to tag names\n",
    "    decoded_labels = [idx2tag[label.item()] for label in filtered_labels if label != -100]\n",
    "\n",
    "    for token, label in zip(decoded_tokens, decoded_labels):\n",
    "        print(f\"{token}: {label}\")\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "tag_values = list(set(tag for doc in train_tags for tag in doc))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tag_values)}\n",
    "idx2tag = dict([(value, key) for key, value in tag2idx.items()])\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_data = NERDataset(train_tokens, train_tags, tokenizer, MAX_LEN, tag2idx)\n",
    "\n",
    "# Check the first item in the dataset\n",
    "item = train_data[1]\n",
    "\n",
    "# Print each word with its tag\n",
    "print_decoded_words_with_tags(item, tokenizer, idx2tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f9ac607-b7cc-481f-9ec9-5e955bd5f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tokenizer, max_len, tag2idx):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tag2idx = tag2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        word_labels = self.tags[idx]\n",
    "        encoding = self.tokenizer(sentence, is_split_into_words=True, return_offsets_mapping=True, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
    "        labels = [self.tag2idx['O']] * self.max_len  # Initialize labels with \"O\"\n",
    "        offsets = encoding['offset_mapping'].squeeze().tolist()  # Get the offsets\n",
    "        encoding.pop('offset_mapping')  # Remove offsets, not needed for model input\n",
    "\n",
    "        idx = 0\n",
    "        previous_label = self.tag2idx['O']\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            if start == end:  # Special tokens\n",
    "                labels[i] = self.tag2idx['O']\n",
    "            elif start == 0:  # Start of a new word\n",
    "                if idx < len(word_labels):\n",
    "                    labels[i] = self.tag2idx[word_labels[idx]]\n",
    "                    previous_label = labels[i]\n",
    "                else:\n",
    "                    labels[i] = self.tag2idx['O']\n",
    "                    previous_label = labels[i]\n",
    "                idx += 1\n",
    "            else:  # Subtoken of a word\n",
    "                if previous_label == self.tag2idx['B-CHAR'] or previous_label == self.tag2idx['I-CHAR']:\n",
    "                    labels[i] = self.tag2idx.get('I-CHAR', -100)\n",
    "                else:\n",
    "                    labels[i] = -100  # PyTorch's convention to ignore these tokens in loss computation\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}  # Remove batch dimension\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "def print_decoded_words_with_tags(item, tokenizer, idx2tag):\n",
    "    input_ids = item['input_ids']\n",
    "    attention_mask = item['attention_mask']\n",
    "    labels = item['labels']\n",
    "\n",
    "    # Decode the input_ids\n",
    "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Map label indices back to tag names\n",
    "    decoded_labels = [idx2tag[label.item()] if label != -100 else 'IGN' for label in labels]\n",
    "\n",
    "    for token, label, mask in zip(decoded_tokens, decoded_labels, attention_mask):\n",
    "        if mask == 1:\n",
    "            print(f\"{token}: {label}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d91c03a8-2959-4b71-82bf-3a3947b0bbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: O\n",
      "further: O\n",
      "information: O\n",
      "will: O\n",
      "also: O\n",
      "be: O\n",
      "found: O\n",
      "in: O\n",
      "the: O\n",
      "selection: O\n",
      "from: O\n",
      "the: O\n",
      "red: O\n",
      "book: O\n",
      "of: O\n",
      "west: B-LOC\n",
      "##mar: IGN\n",
      "##ch: IGN\n",
      "that: O\n",
      "has: O\n",
      "already: O\n",
      "been: O\n",
      "published: O\n",
      "under: O\n",
      "the: O\n",
      "title: O\n",
      "of: O\n",
      "the: O\n",
      "ho: B-CHAR\n",
      "##bb: I-CHAR\n",
      "##it: I-CHAR\n",
      "[SEP]: O\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "tag_values = list(set(tag for doc in train_tags for tag in doc))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tag_values)}\n",
    "idx2tag = dict([(value, key) for key, value in tag2idx.items()])\n",
    "MAX_LEN = 128\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_data = NERDataset(train_tokens, train_tags, tokenizer, MAX_LEN, tag2idx)\n",
    "# Check the first item in the dataset\n",
    "item = train_data[1]\n",
    "\n",
    "# Print each word with its tag\n",
    "print_decoded_words_with_tags(item, tokenizer, idx2tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5002fffb-749b-4864-8a49-b14dc8d3b56d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_tokens\u001b[49m],\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tags) \u001b[38;5;28;01mfor\u001b[39;00m tags \u001b[38;5;129;01min\u001b[39;00m train_tags]}\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091da4e0-cd14-4a0d-b499-9e7e4f49e89b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931553d-92b3-4dfc-8523-d2a229a1d7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c36c1f8e-15c2-4b63-8644-447d854c047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                             is_pretokenized=True,\n",
    "                             return_offsets_mapping=True,\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             max_length=self.max_len)\n",
    "\n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "\n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "          if mapping[0] == 0 and mapping[1] != 0:\n",
    "            # overwrite label\n",
    "            encoded_labels[idx] = labels[i]\n",
    "            i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae08f12d-8569-4394-804e-8709d38e838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = dataset(df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e01601c-8ae0-4e03-aa63-12770d5c65e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'is_pretokenized'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[53], line 15\u001b[0m, in \u001b[0;36mdataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m word_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mword_labels[index]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# step 3: create token labels only for first word pieces of each tokenized word\u001b[39;00m\n\u001b[1;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m [labels_to_ids[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m word_labels]\n",
      "File \u001b[0;32m~/.conda/envs/bert-try/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.conda/envs/bert-try/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2944\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2940\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2941\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2942\u001b[0m         )\n\u001b[1;32m   2943\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2946\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2965\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2966\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2982\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2983\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/bert-try/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3135\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3127\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3128\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3132\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3133\u001b[0m )\n\u001b[0;32m-> 3135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'is_pretokenized'"
     ]
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d50bd-8de5-49c1-a4ad-18d65aa307c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11fe6ace-21d0-4b21-ae81-83284c7dfeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'largely',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'hobbits',\n",
       " 'and',\n",
       " 'from',\n",
       " 'its',\n",
       " 'pages',\n",
       " 'a',\n",
       " 'reader',\n",
       " 'may',\n",
       " 'discover',\n",
       " 'much',\n",
       " 'of',\n",
       " 'their',\n",
       " 'character',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'of',\n",
       " 'their',\n",
       " 'history']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee9d7ad2-78ff-433c-9ded-df094b11190f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-CHAR',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d46f6-36e3-49e0-bb8e-aa24ad8cf23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bef7ad91-99d1-4903-af47-38a3344bc26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  2023,  2338,  2003,  4321,  4986,  2007,  7570, 10322, 12762,\n",
      "         1998,  2013,  2049,  5530,  1037,  8068,  2089,  7523,  2172,  1997,\n",
      "         2037,  2839,  1998,  1037,  2210,  1997,  2037,  2381,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([3, 3, 3, 3, 3, 3, 3, 2, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3])}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16364a5f-a8f2-48a1-985c-386fadb3b88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ba833-81ef-4a0a-bbe6-f74130c68ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc2f00-2d2a-4772-93fd-d80bbf3b396e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3212975-7b06-4906-908a-5a5a8cb331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverted_sentence[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da15fd-1c55-4763-bc20-02b79778f435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0ab3e-1b4f-43c0-9fc7-1897dba10532",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008fe83-2252-41e0-af42-0becd9d95117",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735cbff-62e1-4a5a-9dfe-df017d7e0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4e558-ca4d-4d73-bb2a-bfa00ac3985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(tag_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0c06c-099c-45db-a5a8-c1fdce4d7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_added_toks > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372521a-0a9e-4cd8-8838-5a01641cd9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the test data\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "\n",
    "# Step 2: Create a DataLoader for the test data\n",
    "test_data = NERDataset(test_tokens, test_tags, tokenizer, MAX_LEN, tag2idx)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099f710-2bb2-4212-b929-cbc1b091f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d304a-700b-4f99-bb0e-72e67cc8cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('ner_model_from_final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4a5d0-cc7e-408f-af28-1caf9f644483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predictions = predict(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfbd4fa-5deb-4de7-879a-8b7657f2e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fc2a1-ae22-4f1b-906d-f4b2a046c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = [[idx2tag[idx] for idx in pred] for pred in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148cc85-d642-425e-b1a6-0b84e4cd2373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to the file\n",
    "def write_predictions_to_file(predictions, tokens, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, sentence in enumerate(tokens):\n",
    "            for j, word in enumerate(sentence):\n",
    "                pred_tag = predictions[i][j]\n",
    "                f.write(f\"{j+1}\\t{word}\\t{pred_tag}\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c6235-0b50-4cbc-9df3-59739a43541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_predictions_to_file(predicted_tags, test_tokens, 'predictions.iob2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99a84c-911d-49c2-83b9-758a924ce6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1dc53a-894c-45fa-8013-d06bd7347257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e38042-420f-498c-9270-a75c13307454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767ba13-a55c-4d03-9fa7-781a58a8915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss, f1, accuracy, true_labels_tags, true_preds_tags = evaluate(model, test_loader, device, tag2idx, idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f59ba-3f72-4bd5-8b25-2683f0837ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final Test Loss: {avg_loss}\")\n",
    "print(f\"Final Test F1 Score: {f1}\")\n",
    "print(f\"Final Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c25f63-11c6-4016-9c6b-f6eceafc8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('ner_model_from_final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd36d7-b564-41ed-8709-5638c7596d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "all_preds = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389b3fe-dd04-4714-a2b4-2418568d1647",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "        # Collect the predictions and true labels for calculating F1 score and accuracy\n",
    "        all_preds.extend(predictions.cpu().numpy().tolist())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054b81c-2284-48b4-bd4c-80270c7089a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_flat = [l for labels in all_labels for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb06d04-5a90-46f2-be01-e6d6c42c3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94062c3-763a-4012-bc0f-8571226c0677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-try)",
   "language": "python",
   "name": "bert-try"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
