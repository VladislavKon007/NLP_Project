{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191a16ad-034b-426d-986d-5b04a623d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogu/.conda/envs/bert-try/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import torch.nn.functional as F \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3b5cf8-8570-4fb2-b0d4-7953d271b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751f6f14-788b-4987-a20f-c1579e0f85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 174\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43fbc7f-5bd3-46b8-8d66-c9661497880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {'B-CHAR': 0, 'I-LOC': 1, 'I-CHAR': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc3e16d-e3ca-45d7-a9be-3ae455547471",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_labels = {0: 'B-CHAR', 1: 'I-LOC', 2: 'I-CHAR', 3: 'O', 4: 'B-ORG', 5: 'B-LOC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de7e07-857b-4101-af3f-a1912899bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
    "ids_to_labels = {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33ea505-6e82-40e0-9af2-38a466e16f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('final_train', num_labels=6)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875bb3c0-525c-47e6-a762-9fd9144743f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"final_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "789c28e6-7572-4bc0-b773-7e0f1da5fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids tensor([[  101,  1051,  9541, 11631, 23644, 10424,  7716,  2080, 13360,  4430,\n",
      "          2232, 21025, 19968,  2072,  2026,  9062,  5256,  2039,  5256,  2039,\n",
      "          5256,  2039,  3637,  3111,  2057,  2442,  2175,  2748,  2057,  2442,\n",
      "          2175,  2012,  2320,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]], device='cuda:0')\n",
      "mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "outputs: TokenClassifierOutput(loss=None, logits=tensor([[[-0.2739, -1.9817,  8.5427, -2.6442, -1.9230, -2.7914],\n",
      "         [ 0.1087, -2.3047,  8.1451, -2.6871, -2.0886, -2.7479],\n",
      "         [-0.5515, -1.8743,  7.6731, -2.1733, -1.4903, -2.4419],\n",
      "         ...,\n",
      "         [-1.5337, -1.8424,  8.5520, -2.2465, -2.1584, -2.1851],\n",
      "         [-1.3581, -1.9553,  8.3979, -2.2741, -2.2404, -2.2148],\n",
      "         [-1.3384, -1.9582,  8.5251, -2.3257, -2.2713, -2.2384]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "logits tensor([[[-0.2739, -1.9817,  8.5427, -2.6442, -1.9230, -2.7914],\n",
      "         [ 0.1087, -2.3047,  8.1451, -2.6871, -2.0886, -2.7479],\n",
      "         [-0.5515, -1.8743,  7.6731, -2.1733, -1.4903, -2.4419],\n",
      "         ...,\n",
      "         [-1.5337, -1.8424,  8.5520, -2.2465, -2.1584, -2.1851],\n",
      "         [-1.3581, -1.9553,  8.3979, -2.2741, -2.2404, -2.2148],\n",
      "         [-1.3384, -1.9582,  8.5251, -2.3257, -2.2713, -2.2384]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "active_logits tensor([[-0.2739, -1.9817,  8.5427, -2.6442, -1.9230, -2.7914],\n",
      "        [ 0.1087, -2.3047,  8.1451, -2.6871, -2.0886, -2.7479],\n",
      "        [-0.5515, -1.8743,  7.6731, -2.1733, -1.4903, -2.4419],\n",
      "        ...,\n",
      "        [-1.5337, -1.8424,  8.5520, -2.2465, -2.1584, -2.1851],\n",
      "        [-1.3581, -1.9553,  8.3979, -2.2741, -2.2404, -2.2148],\n",
      "        [-1.3384, -1.9582,  8.5251, -2.3257, -2.2713, -2.2384]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "flattened_predictions tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2], device='cuda:0')\n",
      "tokens ['[CLS]', 'o', '##oo', '##oh', '##hh', 'fr', '##od', '##o', 'aaa', '##ah', '##h', 'gi', '##ml', '##i', 'my', 'precious', 'wake', 'up', 'wake', 'up', 'wake', 'up', 'sleep', '##ies', 'we', 'must', 'go', 'yes', 'we', 'must', 'go', 'at', 'once', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token_predictions ['I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'B-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR']\n",
      "wp_preds [('[CLS]', 'I-CHAR'), ('o', 'I-CHAR'), ('##oo', 'I-CHAR'), ('##oh', 'I-CHAR'), ('##hh', 'I-CHAR'), ('fr', 'B-CHAR'), ('##od', 'I-CHAR'), ('##o', 'I-CHAR'), ('aaa', 'I-CHAR'), ('##ah', 'I-CHAR'), ('##h', 'I-CHAR'), ('gi', 'B-CHAR'), ('##ml', 'B-CHAR'), ('##i', 'I-CHAR'), ('my', 'I-CHAR'), ('precious', 'I-CHAR'), ('wake', 'I-CHAR'), ('up', 'I-CHAR'), ('wake', 'I-CHAR'), ('up', 'I-CHAR'), ('wake', 'I-CHAR'), ('up', 'I-CHAR'), ('sleep', 'I-CHAR'), ('##ies', 'I-CHAR'), ('we', 'I-CHAR'), ('must', 'I-CHAR'), ('go', 'I-CHAR'), ('yes', 'I-CHAR'), ('we', 'I-CHAR'), ('must', 'I-CHAR'), ('go', 'I-CHAR'), ('at', 'I-CHAR'), ('once', 'I-CHAR'), ('[SEP]', 'B-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR'), ('[PAD]', 'I-CHAR')]\n",
      "['Oooohhh', 'Frodo', 'Aaaahh', 'Gimli', 'My', 'precious', 'Wake', 'up', 'Wake', 'up', 'Wake', 'up', 'sleepies', 'We', 'must', 'go', 'yes', 'we', 'must', 'go', 'at', 'once']\n",
      "['I-CHAR', 'B-CHAR', 'I-CHAR', 'B-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR', 'I-CHAR']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Oooohhh Frodo Aaaahh Gimli My precious Wake up Wake up Wake up sleepies We must go yes we must go at once\"\n",
    "words = sentence.split()\n",
    "inputs = tokenizer(words,\n",
    "             is_split_into_words=True,\n",
    "             return_offsets_mapping=True,\n",
    "             padding='max_length',\n",
    "             truncation=True,\n",
    "             max_length=MAX_LEN,\n",
    "             return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "print('ids', ids)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "print('mask', mask)\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "print('outputs:', outputs)\n",
    "\n",
    "logits = outputs[0]\n",
    "print(\"logits\", logits)\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "print('active_logits', active_logits)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "print('flattened_predictions', flattened_predictions)\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "print('tokens', tokens)\n",
    "\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "print('token_predictions', token_predictions)\n",
    "\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "print('wp_preds', wp_preds)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "  #only predictions on first word pieces are important\n",
    "  if mapping[0] == 0 and mapping[1] != 0:\n",
    "    prediction.append(token_pred[1])\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "print(sentence.split())\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ca0a2-4f74-4f40-9354-4dddcef4cdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca95661-99ba-44c0-96db-bd07e846d3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "563b1965-a031-4216-b3ec-68e502ff8970",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(ids\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Check if predictions were made\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpredicted_labels\u001b[49m\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo predictions made.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m token_predictions \u001b[38;5;241m=\u001b[39m [ids_to_labels[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m flattened_predictions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_labels' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = BertForTokenClassification.from_pretrained(\"curve_train\", num_labels=6)\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('curve_tokenizer')\n",
    "\n",
    "# Text to predict\n",
    "text = \"Oooohhh Frodo Aaaahh Gimli My precious Wake up Wake up Wake up sleepies We must go yes we must go at once\"\n",
    "\n",
    "# Tokenize the input text\n",
    "words = text.split()\n",
    "inputs = tokenizer(words,\n",
    "             is_split_into_words=True,\n",
    "             return_offsets_mapping=True,\n",
    "             padding='max_length',\n",
    "             truncation=True,\n",
    "             max_length=MAX_LEN,\n",
    "             return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=ids, attention_mask=attention_mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels)\n",
    "\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "# Check if predictions were made\n",
    "if predicted_labels.numel() == 0:\n",
    "    print(\"No predictions made.\")\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "  #only predictions on first word pieces are important\n",
    "  if mapping[0] == 0 and mapping[1] != 0:\n",
    "    prediction.append(token_pred[1])\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "print(text.split())\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99169775-cd59-425f-884d-9cf7e7ed5e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.06755576183398565\n",
      "Validation Accuracy: 0.9915406698564593\n",
      "Validation F1-Score: 0.8881453154875717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CHAR       0.87      0.87      0.87       820\n",
      "         LOC       0.91      0.98      0.94       216\n",
      "         ORG       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.88      0.89      0.89      1038\n",
      "   macro avg       0.93      0.78      0.83      1038\n",
      "weighted avg       0.88      0.89      0.89      1038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MODEL_PATH = 'ner_model_from_final1.pth'\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "\n",
    "\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "loss_values, validation_loss_values = [], []   \n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for idx, batch in enumerate(testing_loader):\n",
    "    ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "    mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "    labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(ids, token_type_ids=None, attention_mask=mask, labels=labels)\n",
    "    \n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = labels.to('cpu').numpy()\n",
    "    eval_loss += outputs[0].mean().item()\n",
    "    \n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "# Ensure tag values are correctly ordered\n",
    "tag_values = {0: 'B-CHAR', 1: 'I-LOC', 2: 'I-CHAR', 3: 'O', 4: 'B-ORG', 5: 'B-LOC', -100: 'PAD'}\n",
    "\n",
    "# Converting true labels and predictions to tag names\n",
    "pred_tags = [tag_values[int(p_i)] for p, l in zip(predictions, true_labels) for p_i, l_i in zip(p, l) if int(l_i) != -100]\n",
    "valid_tags = [tag_values[int(l_i)] for l in true_labels for l_i in l if int(l_i) != -100]\n",
    "\n",
    "eval_loss = eval_loss / len(testing_loader)\n",
    "validation_loss_values.append(eval_loss)\n",
    "print(\"Validation loss: {}\".format(eval_loss))\n",
    "valid_tags = [valid_tags]\n",
    "pred_tags = [pred_tags]\n",
    "report = seqeval_classification_report(valid_tags, pred_tags)\n",
    "print(\"Validation Accuracy: {}\".format(accuracy_score(valid_tags, pred_tags)))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags)))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2a5b4b-20db-4127-93c1-e6a06f70920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids tensor([[  101,  1051,  9541, 11631, 23644, 10424,  7716,  2080, 13360,  4430,\n",
      "          2232, 21025, 19968,  2072,  2026,  9062,  5256,  2039,  5256,  2039,\n",
      "          5256,  2039,  3637,  3111,  2057,  2442,  2175,  2748,  2057,  2442,\n",
      "          2175,  2012,  2320,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]], device='cuda:0')\n",
      "mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "outputs: TokenClassifierOutput(loss=None, logits=tensor([[[ 0.0325, -2.5970, -2.0588,  3.7939, -2.5481, -0.2213],\n",
      "         [-3.0135, -4.3430, -3.8220, 10.3548, -4.6490, -2.6327],\n",
      "         [-2.8575, -4.3411, -3.6790, 10.1215, -4.6585, -2.4067],\n",
      "         ...,\n",
      "         [-1.1237, -3.1709, -3.0019,  6.6855, -3.3372, -1.9821],\n",
      "         [-1.6388, -3.5830, -3.4392,  8.0992, -3.7696, -2.6002],\n",
      "         [-1.6495, -3.5284, -3.2052,  7.7773, -3.6614, -2.1440]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "logits tensor([[[ 0.0325, -2.5970, -2.0588,  3.7939, -2.5481, -0.2213],\n",
      "         [-3.0135, -4.3430, -3.8220, 10.3548, -4.6490, -2.6327],\n",
      "         [-2.8575, -4.3411, -3.6790, 10.1215, -4.6585, -2.4067],\n",
      "         ...,\n",
      "         [-1.1237, -3.1709, -3.0019,  6.6855, -3.3372, -1.9821],\n",
      "         [-1.6388, -3.5830, -3.4392,  8.0992, -3.7696, -2.6002],\n",
      "         [-1.6495, -3.5284, -3.2052,  7.7773, -3.6614, -2.1440]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "active_logits tensor([[ 0.0325, -2.5970, -2.0588,  3.7939, -2.5481, -0.2213],\n",
      "        [-3.0135, -4.3430, -3.8220, 10.3548, -4.6490, -2.6327],\n",
      "        [-2.8575, -4.3411, -3.6790, 10.1215, -4.6585, -2.4067],\n",
      "        ...,\n",
      "        [-1.1237, -3.1709, -3.0019,  6.6855, -3.3372, -1.9821],\n",
      "        [-1.6388, -3.5830, -3.4392,  8.0992, -3.7696, -2.6002],\n",
      "        [-1.6495, -3.5284, -3.2052,  7.7773, -3.6614, -2.1440]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "flattened_predictions tensor([3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3,\n",
      "        0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3], device='cuda:0')\n",
      "tokens ['[CLS]', 'o', '##oo', '##oh', '##hh', 'fr', '##od', '##o', 'aaa', '##ah', '##h', 'gi', '##ml', '##i', 'my', 'precious', 'wake', 'up', 'wake', 'up', 'wake', 'up', 'sleep', '##ies', 'we', 'must', 'go', 'yes', 'we', 'must', 'go', 'at', 'once', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token_predictions ['O', 'O', 'O', 'O', 'O', 'B-CHAR', 'B-CHAR', 'O', 'O', 'O', 'O', 'B-CHAR', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "wp_preds [('[CLS]', 'O'), ('o', 'O'), ('##oo', 'O'), ('##oh', 'O'), ('##hh', 'O'), ('fr', 'B-CHAR'), ('##od', 'B-CHAR'), ('##o', 'O'), ('aaa', 'O'), ('##ah', 'O'), ('##h', 'O'), ('gi', 'B-CHAR'), ('##ml', 'B-CHAR'), ('##i', 'O'), ('my', 'O'), ('precious', 'O'), ('wake', 'O'), ('up', 'O'), ('wake', 'O'), ('up', 'O'), ('wake', 'O'), ('up', 'O'), ('sleep', 'O'), ('##ies', 'O'), ('we', 'O'), ('must', 'O'), ('go', 'O'), ('yes', 'O'), ('we', 'O'), ('must', 'O'), ('go', 'O'), ('at', 'O'), ('once', 'O'), ('[SEP]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O')]\n",
      "['Oooohhh', 'Frodo', 'Aaaahh', 'Gimli', 'My', 'precious', 'Wake', 'up', 'Wake', 'up', 'Wake', 'up', 'sleepies', 'We', 'must', 'go', 'yes', 'we', 'must', 'go', 'at', 'once']\n",
      "['O', 'B-CHAR', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Oooohhh Frodo Aaaahh Gimli My precious Wake up Wake up Wake up sleepies We must go yes we must go at once\"\n",
    "words = sentence.split()\n",
    "inputs = tokenizer(words,\n",
    "             is_split_into_words=True,\n",
    "             return_offsets_mapping=True,\n",
    "             padding='max_length',\n",
    "             truncation=True,\n",
    "             max_length=MAX_LEN,\n",
    "             return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "print('ids', ids)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "print('mask', mask)\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "print('outputs:', outputs)\n",
    "\n",
    "logits = outputs[0]\n",
    "print(\"logits\", logits)\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "print('active_logits', active_logits)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "print('flattened_predictions', flattened_predictions)\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "print('tokens', tokens)\n",
    "\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "print('token_predictions', token_predictions)\n",
    "\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "print('wp_preds', wp_preds)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "  #only predictions on first word pieces are important\n",
    "  if mapping[0] == 0 and mapping[1] != 0:\n",
    "    prediction.append(token_pred[1])\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "print(sentence.split())\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93feafa8-c7a5-4cc2-ad22-316d99cc2791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-try)",
   "language": "python",
   "name": "bert-try"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
