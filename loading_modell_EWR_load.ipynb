{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191a16ad-034b-426d-986d-5b04a623d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogu/.conda/envs/bert-try/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3b5cf8-8570-4fb2-b0d4-7953d271b1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751f6f14-788b-4987-a20f-c1579e0f85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 174\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0de7e07-857b-4101-af3f-a1912899bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Labels for EWT-LOTR\n",
    "# labels_to_ids = {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
    "# ids_to_labels = {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6628df8-0566-410e-bcf6-28fb3b5b2ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_to_ids: {'B-CHAR': 0, 'I-LOC': 1, 'I-CHAR': 2, 'O': 3, 'B-ORG': 4, 'B-LOC': 5}\n",
      "ids_to_labels: {0: 'B-CHAR', 1: 'I-LOC', 2: 'I-CHAR', 3: 'O', 4: 'B-ORG', 5: 'B-LOC'}\n"
     ]
    }
   ],
   "source": [
    "# Load the config.json file\n",
    "config_path = 'curve_train/config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Extract label2id and id2label and convert keys to integers\n",
    "labels_to_ids = config['label2id']\n",
    "ids_to_labels = {int(k): v for k, v in config['id2label'].items()}\n",
    "\n",
    "# Print the extracted mappings (optional)\n",
    "print(\"labels_to_ids:\", labels_to_ids)\n",
    "print(\"ids_to_labels:\", ids_to_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378cf7b2-c65d-4f2d-9598-13b85c28d55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dadbbdf-ad57-4399-9e8a-50bc404bb3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ea505-6e82-40e0-9af2-38a466e16f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with the extracted labels\n",
    "model = BertForTokenClassification.from_pretrained('curve_train', num_labels=len(ids_to_labels))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875bb3c0-525c-47e6-a762-9fd9144743f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"final_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "789c28e6-7572-4bc0-b773-7e0f1da5fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids tensor([[  101,  1051,  9541, 11631, 23644, 10424,  7716,  2080, 13360,  4430,\n",
      "          2232, 21025, 19968,  2072,  2026,  9062,  5256,  2039,  5256,  2039,\n",
      "          5256,  2039,  3637,  3111,  2057,  2442,  2175,  2748,  2057,  2442,\n",
      "          2175,  2012,  2320,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]], device='cuda:0')\n",
      "mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "outputs: TokenClassifierOutput(loss=None, logits=tensor([[[ 0.0325, -2.5970, -2.0588,  3.7939, -2.5481, -0.2213],\n",
      "         [-3.0135, -4.3430, -3.8220, 10.3548, -4.6490, -2.6327],\n",
      "         [-2.8575, -4.3411, -3.6790, 10.1215, -4.6585, -2.4067],\n",
      "         ...,\n",
      "         [-1.1237, -3.1709, -3.0019,  6.6855, -3.3372, -1.9821],\n",
      "         [-1.6388, -3.5830, -3.4392,  8.0992, -3.7696, -2.6002],\n",
      "         [-1.6495, -3.5284, -3.2052,  7.7773, -3.6614, -2.1440]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n",
      "logits tensor([[[ 0.0325, -2.5970, -2.0588,  3.7939, -2.5481, -0.2213],\n",
      "         [-3.0135, -4.3430, -3.8220, 10.3548, -4.6490, -2.6327],\n",
      "         [-2.8575, -4.3411, -3.6790, 10.1215, -4.6585, -2.4067],\n",
      "         ...,\n",
      "         [-1.1237, -3.1709, -3.0019,  6.6855, -3.3372, -1.9821],\n",
      "         [-1.6388, -3.5830, -3.4392,  8.0992, -3.7696, -2.6002],\n",
      "         [-1.6495, -3.5284, -3.2052,  7.7773, -3.6614, -2.1440]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "active_logits tensor([[ 0.0325, -2.5970, -2.0588,  3.7939, -2.5481, -0.2213],\n",
      "        [-3.0135, -4.3430, -3.8220, 10.3548, -4.6490, -2.6327],\n",
      "        [-2.8575, -4.3411, -3.6790, 10.1215, -4.6585, -2.4067],\n",
      "        ...,\n",
      "        [-1.1237, -3.1709, -3.0019,  6.6855, -3.3372, -1.9821],\n",
      "        [-1.6388, -3.5830, -3.4392,  8.0992, -3.7696, -2.6002],\n",
      "        [-1.6495, -3.5284, -3.2052,  7.7773, -3.6614, -2.1440]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "flattened_predictions tensor([3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3,\n",
      "        0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3], device='cuda:0')\n",
      "tokens ['[CLS]', 'o', '##oo', '##oh', '##hh', 'fr', '##od', '##o', 'aaa', '##ah', '##h', 'gi', '##ml', '##i', 'my', 'precious', 'wake', 'up', 'wake', 'up', 'wake', 'up', 'sleep', '##ies', 'we', 'must', 'go', 'yes', 'we', 'must', 'go', 'at', 'once', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token_predictions ['O', 'O', 'O', 'O', 'O', 'B-CHAR', 'B-CHAR', 'O', 'O', 'O', 'O', 'B-CHAR', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "wp_preds [('[CLS]', 'O'), ('o', 'O'), ('##oo', 'O'), ('##oh', 'O'), ('##hh', 'O'), ('fr', 'B-CHAR'), ('##od', 'B-CHAR'), ('##o', 'O'), ('aaa', 'O'), ('##ah', 'O'), ('##h', 'O'), ('gi', 'B-CHAR'), ('##ml', 'B-CHAR'), ('##i', 'O'), ('my', 'O'), ('precious', 'O'), ('wake', 'O'), ('up', 'O'), ('wake', 'O'), ('up', 'O'), ('wake', 'O'), ('up', 'O'), ('sleep', 'O'), ('##ies', 'O'), ('we', 'O'), ('must', 'O'), ('go', 'O'), ('yes', 'O'), ('we', 'O'), ('must', 'O'), ('go', 'O'), ('at', 'O'), ('once', 'O'), ('[SEP]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'B-CHAR'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O'), ('[PAD]', 'O')]\n",
      "['Oooohhh', 'Frodo', 'Aaaahh', 'Gimli', 'My', 'precious', 'Wake', 'up', 'Wake', 'up', 'Wake', 'up', 'sleepies', 'We', 'must', 'go', 'yes', 'we', 'must', 'go', 'at', 'once']\n",
      "['O', 'B-CHAR', 'O', 'B-CHAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Oooohhh Frodo Aaaahh Gimli My precious Wake up Wake up Wake up sleepies We must go yes we must go at once\"\n",
    "words = sentence.split()\n",
    "inputs = tokenizer(words,\n",
    "             is_split_into_words=True,\n",
    "             return_offsets_mapping=True,\n",
    "             padding='max_length',\n",
    "             truncation=True,\n",
    "             max_length=MAX_LEN,\n",
    "             return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "print('ids', ids)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "print('mask', mask)\n",
    "# forward pass\n",
    "outputs = model(ids, attention_mask=mask)\n",
    "print('outputs:', outputs)\n",
    "\n",
    "logits = outputs[0]\n",
    "print(\"logits\", logits)\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "print('active_logits', active_logits)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "print('flattened_predictions', flattened_predictions)\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "print('tokens', tokens)\n",
    "\n",
    "token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "print('token_predictions', token_predictions)\n",
    "\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "print('wp_preds', wp_preds)\n",
    "\n",
    "prediction = []\n",
    "for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "  #only predictions on first word pieces are important\n",
    "  if mapping[0] == 0 and mapping[1] != 0:\n",
    "    prediction.append(token_pred[1])\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "print(sentence.split())\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ca0a2-4f74-4f40-9354-4dddcef4cdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca95661-99ba-44c0-96db-bd07e846d3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac2a5b4b-20db-4127-93c1-e6a06f70920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Memory Allocated: 2.55 GB\n",
      "  Memory Cached: 2.91 GB\n",
      "O: 48418\n",
      "B-CHAR: 1269\n",
      "I-CHAR: 328\n",
      "B-LOC: 533\n",
      "B-ORG: 324\n",
      "I-LOC: 74\n",
      "I-ORG: 276\n",
      "Initial tag counts in test_tags: {'O': 48418, 'B-CHAR': 1269, 'I-CHAR': 328, 'B-LOC': 533, 'B-ORG': 324, 'I-LOC': 74, 'I-ORG': 276}\n",
      "Validation loss per 100 evaluation steps: 0.18824134767055511\n",
      "Validation Loss: 0.1471862779178012\n",
      "Validation Accuracy: 0.976338292140096\n",
      "F1 Score: 0.7369226843582548\n",
      "{'CHAR': {'precision': 0.7928007023705005, 'recall': 0.7115839243498818, 'f1-score': 0.7500000000000001, 'support': 1269}, 'LOC': {'precision': 0.8571428571428571, 'recall': 0.8555347091932458, 'f1-score': 0.856338028169014, 'support': 533}, 'ORG': {'precision': 0.6967213114754098, 'recall': 0.2623456790123457, 'f1-score': 0.3811659192825112, 'support': 324}, 'micro avg': {'precision': 0.8053541550474066, 'recall': 0.6792097836312324, 'f1-score': 0.7369226843582548, 'support': 2126}, 'macro avg': {'precision': 0.7822216236629225, 'recall': 0.609821437518491, 'f1-score': 0.6625013158171752, 'support': 2126}, 'weighted avg': {'precision': 0.7942892469818159, 'recall': 0.6792097836312324, 'f1-score': 0.7204496363413069, 'support': 2126}}\n",
      "Eval Loss: 0.1471862779178012, Eval Accuracy: 0.976338292140096\n",
      "{'CHAR': {'precision': 0.7928007023705005, 'recall': 0.7115839243498818, 'f1-score': 0.7500000000000001, 'support': 1269}, 'LOC': {'precision': 0.8571428571428571, 'recall': 0.8555347091932458, 'f1-score': 0.856338028169014, 'support': 533}, 'ORG': {'precision': 0.6967213114754098, 'recall': 0.2623456790123457, 'f1-score': 0.3811659192825112, 'support': 324}, 'micro avg': {'precision': 0.8053541550474066, 'recall': 0.6792097836312324, 'f1-score': 0.7369226843582548, 'support': 2126}, 'macro avg': {'precision': 0.7822216236629225, 'recall': 0.609821437518491, 'f1-score': 0.6625013158171752, 'support': 2126}, 'weighted avg': {'precision': 0.7942892469818159, 'recall': 0.6792097836312324, 'f1-score': 0.7204496363413069, 'support': 2126}}\n",
      "   eval_loss  accuracy  f1_score  \\\n",
      "0   0.147186  0.976338  0.736923   \n",
      "\n",
      "                                              report  \n",
      "0  {'CHAR': {'precision': 0.7928007023705005, 're...  \n",
      "          label  precision    recall  f1-score  support\n",
      "0          CHAR   0.792801  0.711584  0.750000     1269\n",
      "1           LOC   0.857143  0.855535  0.856338      533\n",
      "2           ORG   0.696721  0.262346  0.381166      324\n",
      "3     micro avg   0.805354  0.679210  0.736923     2126\n",
      "4     macro avg   0.782222  0.609821  0.662501     2126\n",
      "5  weighted avg   0.794289  0.679210  0.720450     2126\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "test_tokens, test_tags = read_data(\"./all_test.iob2\")\n",
    "\n",
    "def replace_per_tags(tags_list):\n",
    "    updated_tags_list = []\n",
    "    for tags in tags_list:\n",
    "        updated_tags = []\n",
    "        for tag in tags:\n",
    "            if tag == \"B-PER\":\n",
    "                updated_tags.append(\"B-CHAR\")\n",
    "            elif tag == \"I-PER\":\n",
    "                updated_tags.append(\"I-CHAR\")\n",
    "            else:\n",
    "                updated_tags.append(tag)\n",
    "        updated_tags_list.append(updated_tags)\n",
    "    return updated_tags_list\n",
    "\n",
    "test_tags = replace_per_tags(test_tags)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in test_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts = dict(tag_counts)\n",
    "\n",
    "# Print the counts for each tag\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "#ewt+lotr\n",
    "# labels_to_ids = {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
    "# ids_to_labels = {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
    "\n",
    "# EWT\n",
    "# labels_to_ids = {'I-LOC': 0, 'B-CHAR': 1, 'O': 2, 'B-LOC': 3, 'I-CHAR': 4, 'I-ORG': 5, 'B-ORG': 6}\n",
    "# ids_to_labels = {0: 'I-LOC', 1: 'B-CHAR', 2: 'O', 3: 'B-LOC', 4: 'I-CHAR', 5: 'I-ORG', 6: 'B-ORG'}\n",
    "\n",
    "# Create training and testing datasets\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93feafa8-c7a5-4cc2-ad22-316d99cc2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df.to_csv('data/LOTR_on_all_tags_metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05a530d6-f9ee-480c-82f7-78f8cf2ca5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports_df.to_csv('data/LOTR_on_all_tags_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e670c1-c9ae-4aac-86e5-3f116c0e9dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc933d3-eb32-486e-9592-441fdab968d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d3fc4-61ce-4752-b586-802be003a5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6230f272-a167-4fc4-b92a-444df1ab1896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Memory Allocated: 1.74 GB\n",
      "  Memory Cached: 2.53 GB\n",
      "O: 48418\n",
      "B-CHAR: 1269\n",
      "I-CHAR: 328\n",
      "B-LOC: 533\n",
      "B-ORG: 324\n",
      "I-LOC: 74\n",
      "I-ORG: 276\n",
      "Initial tag counts in test_tags: {'O': 48418, 'B-CHAR': 1269, 'I-CHAR': 328, 'B-LOC': 533, 'B-ORG': 324, 'I-LOC': 74, 'I-ORG': 276}\n",
      "Validation loss per 100 evaluation steps: 0.19724011421203613\n",
      "Validation Loss: 0.2985211599331636\n",
      "Validation Accuracy: 0.9614618718519387\n",
      "F1 Score: 0.5350760202720726\n",
      "{'CHAR': {'precision': 0.7712820512820513, 'recall': 0.4867313915857605, 'f1-score': 0.5968253968253968, 'support': 1545}, 'LOC': {'precision': 0.6738544474393531, 'recall': 0.46904315196998125, 'f1-score': 0.5530973451327434, 'support': 533}, 'ORG': {'precision': 1.0, 'recall': 0.0030864197530864196, 'f1-score': 0.006153846153846154, 'support': 324}, 'micro avg': {'precision': 0.7446176688938382, 'recall': 0.41756869275603664, 'f1-score': 0.5350760202720726, 'support': 2402}, 'macro avg': {'precision': 0.8150454995738015, 'recall': 0.31962032110294275, 'f1-score': 0.3853588627039955, 'support': 2402}, 'weighted avg': {'precision': 0.7805142338534323, 'recall': 0.41756869275603664, 'f1-score': 0.5074479472126713, 'support': 2402}}\n",
      "Eval Loss: 0.2985211599331636, Eval Accuracy: 0.9614618718519387\n",
      "{'CHAR': {'precision': 0.7712820512820513, 'recall': 0.4867313915857605, 'f1-score': 0.5968253968253968, 'support': 1545}, 'LOC': {'precision': 0.6738544474393531, 'recall': 0.46904315196998125, 'f1-score': 0.5530973451327434, 'support': 533}, 'ORG': {'precision': 1.0, 'recall': 0.0030864197530864196, 'f1-score': 0.006153846153846154, 'support': 324}, 'micro avg': {'precision': 0.7446176688938382, 'recall': 0.41756869275603664, 'f1-score': 0.5350760202720726, 'support': 2402}, 'macro avg': {'precision': 0.8150454995738015, 'recall': 0.31962032110294275, 'f1-score': 0.3853588627039955, 'support': 2402}, 'weighted avg': {'precision': 0.7805142338534323, 'recall': 0.41756869275603664, 'f1-score': 0.5074479472126713, 'support': 2402}}\n",
      "   eval_loss  accuracy  f1_score  \\\n",
      "0   0.298521  0.961462  0.535076   \n",
      "\n",
      "                                              report  \n",
      "0  {'CHAR': {'precision': 0.7712820512820513, 're...  \n",
      "          label  precision    recall  f1-score  support\n",
      "0          CHAR   0.771282  0.486731  0.596825     1545\n",
      "1           LOC   0.673854  0.469043  0.553097      533\n",
      "2           ORG   1.000000  0.003086  0.006154      324\n",
      "3     micro avg   0.744618  0.417569  0.535076     2402\n",
      "4     macro avg   0.815045  0.319620  0.385359     2402\n",
      "5  weighted avg   0.780514  0.417569  0.507448     2402\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids.get(label, 0) for label in word_labels]  # Assign default value 0 for missing labels\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "test_tokens, test_tags = read_data(\"./all_test.iob2\")\n",
    "\n",
    "def replace_per_tags(tags_list):\n",
    "    updated_tags_list = []\n",
    "    for tags in tags_list:\n",
    "        updated_tags = []\n",
    "        for tag in tags:\n",
    "            if tag == \"B-PER\":\n",
    "                updated_tags.append(\"B-CHAR\")\n",
    "            elif tag == \"I-PER\":\n",
    "                updated_tags.append(\"I-CHAR\")\n",
    "            else:\n",
    "                updated_tags.append(tag)\n",
    "        updated_tags_list.append(updated_tags)\n",
    "    return updated_tags_list\n",
    "\n",
    "test_tags = replace_per_tags(test_tags)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in test_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts = dict(tag_counts)\n",
    "\n",
    "# Print the counts for each tag\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "#ewt+lotr\n",
    "# labels_to_ids = {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
    "# ids_to_labels = {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
    "\n",
    "# EWT\n",
    "# labels_to_ids = {'I-LOC': 0, 'B-CHAR': 1, 'O': 2, 'B-LOC': 3, 'I-CHAR': 4, 'I-ORG': 5, 'B-ORG': 6}\n",
    "# ids_to_labels = {0: 'I-LOC', 1: 'B-CHAR', 2: 'O', 3: 'B-LOC', 4: 'I-CHAR', 5: 'I-ORG', 6: 'B-ORG'}\n",
    "\n",
    "# Create training and testing datasets\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61b993-b900-4b99-9fab-346d259868a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-try)",
   "language": "python",
   "name": "bert-try"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
