{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1011fc3b-8caa-4fe9-bbba-4c71cd07c3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Memory Allocated: 1.67 GB\n",
      "  Memory Cached: 11.97 GB\n",
      "O: 25000\n",
      "B-CHAR: 820\n",
      "I-CHAR: 85\n",
      "B-LOC: 216\n",
      "B-ORG: 2\n",
      "I-LOC: 2\n",
      "labels_to_ids: {'B-CHAR': 0, 'I-CHAR': 1, 'O': 2, 'B-ORG': 3, 'B-LOC': 4, 'I-LOC': 5}\n",
      "ids_to_labels: {0: 'B-CHAR', 1: 'I-CHAR', 2: 'O', 3: 'B-ORG', 4: 'B-LOC', 5: 'I-LOC'}\n",
      "Initial tag counts in test_tags: {'O': 25000, 'B-CHAR': 820, 'I-CHAR': 85, 'B-LOC': 216, 'B-ORG': 2, 'I-LOC': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7, Train Loss: 0.054613624334596336\n",
      "Epoch 2/7, Train Loss: 0.008279851809808404\n",
      "Epoch 3/7, Train Loss: 0.004345378296814598\n",
      "Epoch 4/7, Train Loss: 0.003000743028331121\n",
      "Epoch 5/7, Train Loss: 0.0021651852104789134\n",
      "Epoch 6/7, Train Loss: 0.0016578336803918811\n",
      "Epoch 7/7, Train Loss: 0.0013517654544650131\n",
      "Validation loss per 100 evaluation steps: 0.16483165323734283\n",
      "Validation Loss: 0.06220581477714909\n",
      "Validation Accuracy: 0.9900095693779905\n",
      "F1 Score: 0.8716981132075472\n",
      "{'CHAR': {'precision': 0.8446026097271648, 'recall': 0.8682926829268293, 'f1-score': 0.8562838244137101, 'support': 820}, 'LOC': {'precision': 0.8865546218487395, 'recall': 0.9768518518518519, 'f1-score': 0.9295154185022027, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.8539741219963032, 'recall': 0.8901734104046243, 'f1-score': 0.8716981132075472, 'support': 1038}, 'macro avg': {'precision': 0.9103857438586348, 'recall': 0.7817148449262271, 'f1-score': 0.8174886365275263, 'support': 1038}, 'weighted avg': {'precision': 0.8536319251402725, 'recall': 0.8901734104046243, 'f1-score': 0.8711574178699917, 'support': 1038}}\n",
      "Eval Loss: 0.06220581477714909, Eval Accuracy: 0.9900095693779905\n",
      "{'CHAR': {'precision': 0.8446026097271648, 'recall': 0.8682926829268293, 'f1-score': 0.8562838244137101, 'support': 820}, 'LOC': {'precision': 0.8865546218487395, 'recall': 0.9768518518518519, 'f1-score': 0.9295154185022027, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.8539741219963032, 'recall': 0.8901734104046243, 'f1-score': 0.8716981132075472, 'support': 1038}, 'macro avg': {'precision': 0.9103857438586348, 'recall': 0.7817148449262271, 'f1-score': 0.8174886365275263, 'support': 1038}, 'weighted avg': {'precision': 0.8536319251402725, 'recall': 0.8901734104046243, 'f1-score': 0.8711574178699917, 'support': 1038}}\n",
      "   eval_loss  accuracy  f1_score  \\\n",
      "0   0.062206   0.99001  0.871698   \n",
      "\n",
      "                                              report  \n",
      "0  {'CHAR': {'precision': 0.8446026097271648, 're...  \n",
      "          label  precision    recall  f1-score  support\n",
      "0          CHAR   0.844603  0.868293  0.856284      820\n",
      "1           LOC   0.886555  0.976852  0.929515      216\n",
      "2           ORG   1.000000  0.500000  0.666667        2\n",
      "3     micro avg   0.853974  0.890173  0.871698     1038\n",
      "4     macro avg   0.910386  0.781715  0.817489     1038\n",
      "5  weighted avg   0.853632  0.890173  0.871157     1038\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "# Data Reading and Preprocessing Functions\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in test_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts = dict(tag_counts)\n",
    "\n",
    "# Print the counts for each tag\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "# Create mappings\n",
    "all_tags = [tag for tags in df['word_labels'] for tag in tags.split(\",\")]\n",
    "unique_tags = set(all_tags)\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_tags)}\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "\n",
    "# Display the mappings\n",
    "print(\"labels_to_ids:\", labels_to_ids)\n",
    "print(\"ids_to_labels:\", ids_to_labels)\n",
    "\n",
    "# Create training and testing datasets\n",
    "training_set = dataset(df, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels_to_ids))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af0008b1-0513-481a-a068-66dee0dc5ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tag counts in test_tags: {'O': 337411, 'B-CHAR': 8230, 'B-LOC': 1679, 'I-CHAR': 213, 'B-ORG': 6, 'I-LOC': 10}\n"
     ]
    }
   ],
   "source": [
    "# Count initial tag occurrences in test_tags\n",
    "train_tag_counts = count_tags(train_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(train_tag_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddf6f5aa-be09-461b-afe1-78adf7b981c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'O': 337411,\n",
       "             'B-CHAR': 8230,\n",
       "             'B-LOC': 1679,\n",
       "             'I-CHAR': 213,\n",
       "             'B-ORG': 6,\n",
       "             'I-LOC': 10})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8d865d-1f1e-4aae-af98-d709b8c1926a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('final_tokenizer/tokenizer_config.json',\n",
       " 'final_tokenizer/special_tokens_map.json',\n",
       " 'final_tokenizer/vocab.txt',\n",
       " 'final_tokenizer/added_tokens.json',\n",
       " 'final_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"final_train\")\n",
    "tokenizer.save_pretrained('final_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e351a0-fe3a-4ce0-88ad-be2b065038a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load(open('bert_train/config.json'))\n",
    "config['id2label'] = ids_to_labels\n",
    "config['label2id'] = labels_to_ids\n",
    "json.dump(config, open('bert_train/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da965f8-092e-4667-83a4-1bf6aa3643be",
   "metadata": {},
   "source": [
    "Model metrics per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba2887e1-1f09-46cb-a068-1c8dc7dde554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/10, Train Loss: 0.05366666917170095\n",
      "Validation loss per 100 evaluation steps: 0.10730549693107605\n",
      "Validation Loss: 0.05126193371704883\n",
      "Validation Accuracy: 0.988133971291866\n",
      "F1 Score: 0.8108360579168612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cogu/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CHAR': {'precision': 0.7551487414187643, 'recall': 0.8048780487804879, 'f1-score': 0.7792207792207793, 'support': 820}, 'LOC': {'precision': 0.9082969432314411, 'recall': 0.9629629629629629, 'f1-score': 0.9348314606741573, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.7869446962828649, 'recall': 0.8362235067437379, 'f1-score': 0.8108360579168612, 'support': 1038}, 'macro avg': {'precision': 0.5544818948834017, 'recall': 0.5892803372478169, 'f1-score': 0.5713507466316455, 'support': 1038}, 'weighted avg': {'precision': 0.7855627241824451, 'recall': 0.8362235067437379, 'f1-score': 0.8101008039177813, 'support': 1038}}\n",
      "Epoch 1/10, Eval Loss: 0.05126193371704883, Eval Accuracy: 0.988133971291866\n",
      "Epoch 2/10, Train Loss: 0.008632887777305878\n",
      "Validation loss per 100 evaluation steps: 0.11779356002807617\n",
      "Validation Loss: 0.04939408013597131\n",
      "Validation Accuracy: 0.9895885167464115\n",
      "F1 Score: 0.850984067478913\n",
      "{'CHAR': {'precision': 0.8025258323765786, 'recall': 0.8524390243902439, 'f1-score': 0.826729745712596, 'support': 820}, 'LOC': {'precision': 0.9288888888888889, 'recall': 0.9675925925925926, 'f1-score': 0.9478458049886621, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.8284671532846716, 'recall': 0.8747591522157996, 'f1-score': 0.850984067478913, 'support': 1038}, 'macro avg': {'precision': 0.5771382404218225, 'recall': 0.6066772056609455, 'f1-score': 0.591525183567086, 'support': 1038}, 'weighted avg': {'precision': 0.8272747423398791, 'recall': 0.8747591522157996, 'f1-score': 0.8503401593081692, 'support': 1038}}\n",
      "Epoch 2/10, Eval Loss: 0.04939408013597131, Eval Accuracy: 0.9895885167464115\n",
      "Epoch 3/10, Train Loss: 0.004702091451540031\n",
      "Validation loss per 100 evaluation steps: 0.11721077561378479\n",
      "Validation Loss: 0.04956226304639131\n",
      "Validation Accuracy: 0.9903923444976076\n",
      "F1 Score: 0.8676401318888366\n",
      "{'CHAR': {'precision': 0.8257839721254355, 'recall': 0.8670731707317073, 'f1-score': 0.8459250446162998, 'support': 820}, 'LOC': {'precision': 0.9375, 'recall': 0.9722222222222222, 'f1-score': 0.9545454545454546, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.8488479262672811, 'recall': 0.8872832369942196, 'f1-score': 0.8676401318888366, 'support': 1038}, 'macro avg': {'precision': 0.5877613240418119, 'recall': 0.6130984643179765, 'f1-score': 0.6001568330539181, 'support': 1038}, 'weighted avg': {'precision': 0.847440132122213, 'recall': 0.8872832369942196, 'f1-score': 0.8668982223190597, 'support': 1038}}\n",
      "Epoch 3/10, Eval Loss: 0.04956226304639131, Eval Accuracy: 0.9903923444976076\n",
      "Epoch 4/10, Train Loss: 0.0030039201059028696\n",
      "Validation loss per 100 evaluation steps: 0.12147748470306396\n",
      "Validation Loss: 0.05035209223214123\n",
      "Validation Accuracy: 0.990468899521531\n",
      "F1 Score: 0.8669201520912547\n",
      "{'CHAR': {'precision': 0.8379022646007152, 'recall': 0.8573170731707317, 'f1-score': 0.8474984930681133, 'support': 820}, 'LOC': {'precision': 0.920704845814978, 'recall': 0.9675925925925926, 'f1-score': 0.9435665914221218, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.8555347091932458, 'recall': 0.8786127167630058, 'f1-score': 0.8669201520912547, 'support': 1038}, 'macro avg': {'precision': 0.5862023701385644, 'recall': 0.6083032219211081, 'f1-score': 0.5970216948300784, 'support': 1038}, 'weighted avg': {'precision': 0.8535184043050306, 'recall': 0.8786127167630058, 'f1-score': 0.8658565973632286, 'support': 1038}}\n",
      "Epoch 4/10, Eval Loss: 0.05035209223214123, Eval Accuracy: 0.990468899521531\n",
      "Epoch 5/10, Train Loss: 0.0022044077128261642\n",
      "Validation loss per 100 evaluation steps: 0.121419258415699\n",
      "Validation Loss: 0.05155141158546839\n",
      "Validation Accuracy: 0.991043062200957\n",
      "F1 Score: 0.8813397129186602\n",
      "{'CHAR': {'precision': 0.8609431680773881, 'recall': 0.8682926829268293, 'f1-score': 0.864602307225258, 'support': 820}, 'LOC': {'precision': 0.9285714285714286, 'recall': 0.9629629629629629, 'f1-score': 0.9454545454545454, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.8754752851711026, 'recall': 0.8872832369942196, 'f1-score': 0.8813397129186602, 'support': 1038}, 'macro avg': {'precision': 0.9298381988829388, 'recall': 0.7770852152965974, 'f1-score': 0.8255745064488234, 'support': 1038}, 'weighted avg': {'precision': 0.8752840331357291, 'recall': 0.8872832369942196, 'f1-score': 0.8810456715570586, 'support': 1038}}\n",
      "Epoch 5/10, Eval Loss: 0.05155141158546839, Eval Accuracy: 0.991043062200957\n",
      "Epoch 6/10, Train Loss: 0.0016639031850581598\n",
      "Validation loss per 100 evaluation steps: 0.12569671869277954\n",
      "Validation Loss: 0.0549511871404118\n",
      "Validation Accuracy: 0.99088995215311\n",
      "F1 Score: 0.8801534036433365\n",
      "{'CHAR': {'precision': 0.8621951219512195, 'recall': 0.8621951219512195, 'f1-score': 0.8621951219512194, 'support': 820}, 'LOC': {'precision': 0.9288888888888889, 'recall': 0.9675925925925926, 'f1-score': 0.9478458049886621, 'support': 216}, 'ORG': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2}, 'micro avg': {'precision': 0.8759541984732825, 'recall': 0.884393063583815, 'f1-score': 0.8801534036433365, 'support': 1038}, 'macro avg': {'precision': 0.8192502258355917, 'recall': 0.943262571514604, 'f1-score': 0.8700136423132939, 'support': 1038}, 'weighted avg': {'precision': 0.8756968529222865, 'recall': 0.884393063583815, 'f1-score': 0.8798985490149815, 'support': 1038}}\n",
      "Epoch 6/10, Eval Loss: 0.0549511871404118, Eval Accuracy: 0.99088995215311\n",
      "Epoch 7/10, Train Loss: 0.001162669700910567\n",
      "Validation loss per 100 evaluation steps: 0.13778796792030334\n",
      "Validation Loss: 0.0613821331722041\n",
      "Validation Accuracy: 0.9911196172248804\n",
      "F1 Score: 0.8813886210221793\n",
      "{'CHAR': {'precision': 0.8616891064871481, 'recall': 0.8585365853658536, 'f1-score': 0.8601099572388515, 'support': 820}, 'LOC': {'precision': 0.9585253456221198, 'recall': 0.9629629629629629, 'f1-score': 0.9607390300230947, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 0.8822393822393823, 'recall': 0.8805394990366089, 'f1-score': 0.8813886210221793, 'support': 1038}, 'macro avg': {'precision': 0.9400714840364226, 'recall': 0.9404998494429387, 'f1-score': 0.9402829957539821, 'support': 1038}, 'weighted avg': {'precision': 0.8821064951578415, 'recall': 0.8805394990366089, 'f1-score': 0.8813196487676751, 'support': 1038}}\n",
      "Epoch 7/10, Eval Loss: 0.0613821331722041, Eval Accuracy: 0.9911196172248804\n",
      "Epoch 8/10, Train Loss: 0.0007936227866246403\n",
      "Validation loss per 100 evaluation steps: 0.15478405356407166\n",
      "Validation Loss: 0.07200890151369903\n",
      "Validation Accuracy: 0.9911961722488039\n",
      "F1 Score: 0.8816793893129771\n",
      "{'CHAR': {'precision': 0.8559423769507803, 'recall': 0.8695121951219512, 'f1-score': 0.8626739261947974, 'support': 820}, 'LOC': {'precision': 0.9372197309417041, 'recall': 0.9675925925925926, 'f1-score': 0.9521640091116174, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 0.8733459357277883, 'recall': 0.8901734104046243, 'f1-score': 0.8816793893129771, 'support': 1038}, 'macro avg': {'precision': 0.9310540359641615, 'recall': 0.9457015959048479, 'f1-score': 0.938279311768805, 'support': 1038}, 'weighted avg': {'precision': 0.873133151236077, 'recall': 0.8901734104046243, 'f1-score': 0.8815607374256679, 'support': 1038}}\n",
      "Epoch 8/10, Eval Loss: 0.07200890151369903, Eval Accuracy: 0.9911961722488039\n",
      "Epoch 9/10, Train Loss: 0.0008303392046603157\n",
      "Validation loss per 100 evaluation steps: 0.1913347691297531\n",
      "Validation Loss: 0.07484324264029661\n",
      "Validation Accuracy: 0.9903923444976076\n",
      "F1 Score: 0.8744588744588744\n",
      "{'CHAR': {'precision': 0.8553921568627451, 'recall': 0.8512195121951219, 'f1-score': 0.8533007334963324, 'support': 820}, 'LOC': {'precision': 0.9375, 'recall': 0.9722222222222222, 'f1-score': 0.9545454545454546, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.8731988472622478, 'recall': 0.8757225433526011, 'f1-score': 0.8744588744588744, 'support': 1038}, 'macro avg': {'precision': 0.9309640522875817, 'recall': 0.7744805781391148, 'f1-score': 0.8248376182361512, 'support': 1038}, 'weighted avg': {'precision': 0.8727568098530356, 'recall': 0.8757225433526011, 'f1-score': 0.8740093959365549, 'support': 1038}}\n",
      "Epoch 9/10, Eval Loss: 0.07484324264029661, Eval Accuracy: 0.9903923444976076\n",
      "Epoch 10/10, Train Loss: 0.0009857384133179358\n",
      "Validation loss per 100 evaluation steps: 0.1919211745262146\n",
      "Validation Loss: 0.07430818087110917\n",
      "Validation Accuracy: 0.9906985645933014\n",
      "F1 Score: 0.8768567321514136\n",
      "{'CHAR': {'precision': 0.8500604594921403, 'recall': 0.8573170731707317, 'f1-score': 0.8536733454766241, 'support': 820}, 'LOC': {'precision': 0.9545454545454546, 'recall': 0.9722222222222222, 'f1-score': 0.963302752293578, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 0.8722592945662536, 'recall': 0.8815028901734104, 'f1-score': 0.8768567321514136, 'support': 1038}, 'macro avg': {'precision': 0.9348686380125316, 'recall': 0.9431797651309847, 'f1-score': 0.9389920325900674, 'support': 1038}, 'weighted avg': {'precision': 0.8720919026641362, 'recall': 0.8815028901734104, 'f1-score': 0.8767683408345325, 'support': 1038}}\n",
      "Epoch 10/10, Eval Loss: 0.07430818087110917, Eval Accuracy: 0.9906985645933014\n",
      "   epoch  train_loss  eval_loss  accuracy  f1_score\n",
      "0      1    0.053667   0.051262  0.988134  0.810836\n",
      "1      2    0.008633   0.049394  0.989589  0.850984\n",
      "2      3    0.004702   0.049562  0.990392  0.867640\n",
      "3      4    0.003004   0.050352  0.990469  0.866920\n",
      "4      5    0.002204   0.051551  0.991043  0.881340\n",
      "5      6    0.001664   0.054951  0.990890  0.880153\n",
      "6      7    0.001163   0.061382  0.991120  0.881389\n",
      "7      8    0.000794   0.072009  0.991196  0.881679\n",
      "8      9    0.000830   0.074843  0.990392  0.874459\n",
      "9     10    0.000986   0.074308  0.990699  0.876857\n",
      "    epoch         label  precision    recall  f1-score  support\n",
      "0       1          CHAR   0.755149  0.804878  0.779221      820\n",
      "1       1           LOC   0.908297  0.962963  0.934831      216\n",
      "2       1           ORG   0.000000  0.000000  0.000000        2\n",
      "3       1     micro avg   0.786945  0.836224  0.810836     1038\n",
      "4       1     macro avg   0.554482  0.589280  0.571351     1038\n",
      "5       1  weighted avg   0.785563  0.836224  0.810101     1038\n",
      "6       2          CHAR   0.802526  0.852439  0.826730      820\n",
      "7       2           LOC   0.928889  0.967593  0.947846      216\n",
      "8       2           ORG   0.000000  0.000000  0.000000        2\n",
      "9       2     micro avg   0.828467  0.874759  0.850984     1038\n",
      "10      2     macro avg   0.577138  0.606677  0.591525     1038\n",
      "11      2  weighted avg   0.827275  0.874759  0.850340     1038\n",
      "12      3          CHAR   0.825784  0.867073  0.845925      820\n",
      "13      3           LOC   0.937500  0.972222  0.954545      216\n",
      "14      3           ORG   0.000000  0.000000  0.000000        2\n",
      "15      3     micro avg   0.848848  0.887283  0.867640     1038\n",
      "16      3     macro avg   0.587761  0.613098  0.600157     1038\n",
      "17      3  weighted avg   0.847440  0.887283  0.866898     1038\n",
      "18      4          CHAR   0.837902  0.857317  0.847498      820\n",
      "19      4           LOC   0.920705  0.967593  0.943567      216\n",
      "20      4           ORG   0.000000  0.000000  0.000000        2\n",
      "21      4     micro avg   0.855535  0.878613  0.866920     1038\n",
      "22      4     macro avg   0.586202  0.608303  0.597022     1038\n",
      "23      4  weighted avg   0.853518  0.878613  0.865857     1038\n",
      "24      5          CHAR   0.860943  0.868293  0.864602      820\n",
      "25      5           LOC   0.928571  0.962963  0.945455      216\n",
      "26      5           ORG   1.000000  0.500000  0.666667        2\n",
      "27      5     micro avg   0.875475  0.887283  0.881340     1038\n",
      "28      5     macro avg   0.929838  0.777085  0.825575     1038\n",
      "29      5  weighted avg   0.875284  0.887283  0.881046     1038\n",
      "30      6          CHAR   0.862195  0.862195  0.862195      820\n",
      "31      6           LOC   0.928889  0.967593  0.947846      216\n",
      "32      6           ORG   0.666667  1.000000  0.800000        2\n",
      "33      6     micro avg   0.875954  0.884393  0.880153     1038\n",
      "34      6     macro avg   0.819250  0.943263  0.870014     1038\n",
      "35      6  weighted avg   0.875697  0.884393  0.879899     1038\n",
      "36      7          CHAR   0.861689  0.858537  0.860110      820\n",
      "37      7           LOC   0.958525  0.962963  0.960739      216\n",
      "38      7           ORG   1.000000  1.000000  1.000000        2\n",
      "39      7     micro avg   0.882239  0.880539  0.881389     1038\n",
      "40      7     macro avg   0.940071  0.940500  0.940283     1038\n",
      "41      7  weighted avg   0.882106  0.880539  0.881320     1038\n",
      "42      8          CHAR   0.855942  0.869512  0.862674      820\n",
      "43      8           LOC   0.937220  0.967593  0.952164      216\n",
      "44      8           ORG   1.000000  1.000000  1.000000        2\n",
      "45      8     micro avg   0.873346  0.890173  0.881679     1038\n",
      "46      8     macro avg   0.931054  0.945702  0.938279     1038\n",
      "47      8  weighted avg   0.873133  0.890173  0.881561     1038\n",
      "48      9          CHAR   0.855392  0.851220  0.853301      820\n",
      "49      9           LOC   0.937500  0.972222  0.954545      216\n",
      "50      9           ORG   1.000000  0.500000  0.666667        2\n",
      "51      9     micro avg   0.873199  0.875723  0.874459     1038\n",
      "52      9     macro avg   0.930964  0.774481  0.824838     1038\n",
      "53      9  weighted avg   0.872757  0.875723  0.874009     1038\n",
      "54     10          CHAR   0.850060  0.857317  0.853673      820\n",
      "55     10           LOC   0.954545  0.972222  0.963303      216\n",
      "56     10           ORG   1.000000  1.000000  1.000000        2\n",
      "57     10     micro avg   0.872259  0.881503  0.876857     1038\n",
      "58     10     macro avg   0.934869  0.943180  0.938992     1038\n",
      "59     10  weighted avg   0.872092  0.881503  0.876768     1038\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score as seq_f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Assuming replace_per_tags, read_data, and dataset are defined as in previous steps\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1 = seq_f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1, report\n",
    "\n",
    "\n",
    "# Lists to store metrics for each epoch\n",
    "all_metrics = []\n",
    "all_reports = []\n",
    "\n",
    "# Training and evaluation for each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "    \n",
    "    eval_loss, eval_accuracy, F1, eval_report = valid(model, testing_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "    \n",
    "    # Store the metrics for this epoch\n",
    "    metrics = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"accuracy\": eval_accuracy,\n",
    "        \"f1_score\": F1\n",
    "    }\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    # Flatten the classification report for this epoch\n",
    "    flat_report = []\n",
    "    for label, scores in eval_report.items():\n",
    "        flat_report.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"label\": label,\n",
    "            \"precision\": scores[\"precision\"],\n",
    "            \"recall\": scores[\"recall\"],\n",
    "            \"f1-score\": scores[\"f1-score\"],\n",
    "            \"support\": scores[\"support\"]\n",
    "        })\n",
    "    all_reports.extend(flat_report)\n",
    "\n",
    "# Convert the lists of metrics and reports to DataFrames\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "reports_df = pd.DataFrame(all_reports)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(metrics_df)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "736e114a-430f-4841-b2b9-5041a40f9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/model-metrics-epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3479dbbd-aad7-464c-ad84-02ec9633276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.to_csv('data/model-reports-epoch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1eeb9c-e275-4fd7-ae87-146b7a8a1a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce0c86-b8ef-4788-959e-a1592e0cdc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e68d3861-f6dc-4037-956e-19e9ea906b03",
   "metadata": {},
   "source": [
    "Trying to do EWT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e16a6146-92ef-4bfe-b303-2ebe974441c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Memory Allocated: 4.12 GB\n",
      "  Memory Cached: 11.97 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "# Data Reading and Preprocessing Functions\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./en_ewt-ud-train.iob2\")\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "def replace_per_tags(tags_list):\n",
    "    updated_tags_list = []\n",
    "    for tags in tags_list:\n",
    "        updated_tags = []\n",
    "        for tag in tags:\n",
    "            if tag == \"B-PER\":\n",
    "                updated_tags.append(\"B-CHAR\")\n",
    "            elif tag == \"I-PER\":\n",
    "                updated_tags.append(\"I-CHAR\")\n",
    "            else:\n",
    "                updated_tags.append(tag)\n",
    "        updated_tags_list.append(updated_tags)\n",
    "    return updated_tags_list\n",
    "\n",
    "train_tags = replace_per_tags(train_tags)\n",
    "\n",
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f81dc11-47aa-43f4-8808-258a7c6b5492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6392463f-6aba-4d5b-99da-2a725b73002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574d841-aae7-4766-9d9b-ae4238eb7c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03e25a05-4697-4aff-af0c-9d69c59a18a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_counts_train: \n",
      "O: 194219\n",
      "B-LOC: 2712\n",
      "I-LOC: 877\n",
      "B-CHAR: 2874\n",
      "B-ORG: 1436\n",
      "I-ORG: 1167\n",
      "I-CHAR: 1294\n",
      "labels_to_ids: {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
      "ids_to_labels: {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts_train = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in train_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts_train[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts_train = dict(tag_counts_train)\n",
    "\n",
    "# Print the counts for each tag\n",
    "print('tag_counts_train: ')\n",
    "for tag, count in tag_counts_train.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "# Create mappings\n",
    "all_tags = [tag for tags in df['word_labels'] for tag in tags.split(\",\")]\n",
    "unique_tags = set(all_tags)\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_tags)}\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "\n",
    "# Display the mappings\n",
    "print(\"labels_to_ids:\", labels_to_ids)\n",
    "print(\"ids_to_labels:\", ids_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67ef1205-bf45-4e90-9e2f-d99ff31bfb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tag counts in test_tags: {'O': 194219, 'B-LOC': 2712, 'I-LOC': 877, 'B-CHAR': 2874, 'B-ORG': 1436, 'I-ORG': 1167, 'I-CHAR': 1294}\n",
      "Initial tag counts in test_tags: {'O': 25000, 'B-CHAR': 820, 'I-CHAR': 85, 'B-LOC': 216, 'B-ORG': 2, 'I-LOC': 2}\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing datasets\n",
    "training_set = dataset(df, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "initial_train_tag_counts = count_tags(train_tags)\n",
    "print(\"Initial tag counts in train_tags:\", dict(initial_train_tag_counts))\n",
    "\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "551b562f-a2c1-4848-ac15-b74214fa96f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7, Train Loss: 0.056734724169757195\n",
      "Epoch 2/7, Train Loss: 0.008274268891156657\n",
      "Epoch 3/7, Train Loss: 0.004442892926945321\n",
      "Epoch 4/7, Train Loss: 0.00292872416117276\n",
      "Epoch 5/7, Train Loss: 0.0021876643148026945\n",
      "Epoch 6/7, Train Loss: 0.00184585696249788\n",
      "Epoch 7/7, Train Loss: 0.0011599590029763235\n",
      "Validation loss per 100 evaluation steps: 0.1601811796426773\n",
      "Validation Loss: 0.061447188542741865\n",
      "Validation Accuracy: 0.9904306220095693\n",
      "F1 Score: 0.8768796992481203\n",
      "{'CHAR': {'precision': 0.8325635103926097, 'recall': 0.8792682926829268, 'f1-score': 0.8552787663107948, 'support': 820}, 'LOC': {'precision': 0.9459459459459459, 'recall': 0.9722222222222222, 'f1-score': 0.9589041095890412, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 0.8559633027522936, 'recall': 0.8988439306358381, 'f1-score': 0.8768796992481203, 'support': 1038}, 'macro avg': {'precision': 0.9261698187795185, 'recall': 0.9504968383017163, 'f1-score': 0.9380609586332787, 'support': 1038}, 'weighted avg': {'precision': 0.8564801568846477, 'recall': 0.8988439306358381, 'f1-score': 0.8771212678671335, 'support': 1038}}\n",
      "Eval Loss: 0.061447188542741865, Eval Accuracy: 0.9904306220095693\n",
      "{'CHAR': {'precision': 0.8325635103926097, 'recall': 0.8792682926829268, 'f1-score': 0.8552787663107948, 'support': 820}, 'LOC': {'precision': 0.9459459459459459, 'recall': 0.9722222222222222, 'f1-score': 0.9589041095890412, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2}, 'micro avg': {'precision': 0.8559633027522936, 'recall': 0.8988439306358381, 'f1-score': 0.8768796992481203, 'support': 1038}, 'macro avg': {'precision': 0.9261698187795185, 'recall': 0.9504968383017163, 'f1-score': 0.9380609586332787, 'support': 1038}, 'weighted avg': {'precision': 0.8564801568846477, 'recall': 0.8988439306358381, 'f1-score': 0.8771212678671335, 'support': 1038}}\n",
      "   eval_loss  accuracy  f1_score  \\\n",
      "0   0.061447  0.990431   0.87688   \n",
      "\n",
      "                                              report  \n",
      "0  {'CHAR': {'precision': 0.8325635103926097, 're...  \n",
      "          label  precision    recall  f1-score  support\n",
      "0          CHAR   0.832564  0.879268  0.855279      820\n",
      "1           LOC   0.945946  0.972222  0.958904      216\n",
      "2           ORG   1.000000  1.000000  1.000000        2\n",
      "3     micro avg   0.855963  0.898844  0.876880     1038\n",
      "4     macro avg   0.926170  0.950497  0.938061     1038\n",
      "5  weighted avg   0.856480  0.898844  0.877121     1038\n"
     ]
    }
   ],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels_to_ids))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddffadcc-e8a2-4e26-8451-61479ae58cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/EWT-training_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "060af902-6f98-453b-b9b7-7ecdb6fca395",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.to_csv('data/EWT-training-report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e3693e7-4886-4117-94b2-527b4e8ef5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EWT-tokenizer/tokenizer_config.json',\n",
       " 'EWT-tokenizer/special_tokens_map.json',\n",
       " 'EWT-tokenizer/vocab.txt',\n",
       " 'EWT-tokenizer/added_tokens.json',\n",
       " 'EWT-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"EWT-baseline\")\n",
    "tokenizer.save_pretrained('EWT-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8100ef1f-2cbc-4889-940d-3d611ff27cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load(open('EWT-baseline/config.json'))\n",
    "config['id2label'] = ids_to_labels\n",
    "config['label2id'] = labels_to_ids\n",
    "json.dump(config, open('EWT-baseline/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "261a64a6-b981-4a42-a04d-2d6ad01fb3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BertForTokenClassification'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f5b47-a79e-465c-bb63-d1c9cbe76660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af4713-092e-4ff0-b1e3-107e95bc5075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef27094e-0f1e-4a72-aa2b-86c17155d27c",
   "metadata": {},
   "source": [
    "EWT: ##Metrics per epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62d0c5e8-9f66-401f-8d37-2e7cb8b62900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/10, Train Loss: 0.003665855062925922\n",
      "Validation loss per 100 evaluation steps: 0.39667797088623047\n",
      "Validation Loss: 0.27037916084130603\n",
      "Validation Accuracy: 0.9535693779904306\n",
      "F1 Score: 0.3712433706540955\n",
      "{'CHAR': {'precision': 0.4827586206896552, 'recall': 0.23902439024390243, 'f1-score': 0.31973898858075034, 'support': 820}, 'LOC': {'precision': 0.5021097046413502, 'recall': 0.5509259259259259, 'f1-score': 0.5253863134657836, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.4779969650986343, 'recall': 0.30346820809248554, 'f1-score': 0.3712433706540955, 'support': 1038}, 'macro avg': {'precision': 0.32828944177700176, 'recall': 0.26331677205660947, 'f1-score': 0.2817084340155113, 'support': 1038}, 'weighted avg': {'precision': 0.4858552650944595, 'recall': 0.30346820809248554, 'f1-score': 0.3619165841472298, 'support': 1038}}\n",
      "Epoch 1/10, Eval Loss: 0.27037916084130603, Eval Accuracy: 0.9535693779904306\n",
      "Epoch 2/10, Train Loss: 0.0027967398999287386\n",
      "Validation loss per 100 evaluation steps: 0.4366508722305298\n",
      "Validation Loss: 0.2802359466751417\n",
      "Validation Accuracy: 0.9515406698564594\n",
      "F1 Score: 0.37556561085972856\n",
      "{'CHAR': {'precision': 0.44835164835164837, 'recall': 0.24878048780487805, 'f1-score': 0.32000000000000006, 'support': 820}, 'LOC': {'precision': 0.5019607843137255, 'recall': 0.5925925925925926, 'f1-score': 0.5435244161358811, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.4547945205479452, 'recall': 0.3198458574181118, 'f1-score': 0.37556561085972856, 'support': 1038}, 'macro avg': {'precision': 0.31677081088845793, 'recall': 0.28045769346582355, 'f1-score': 0.2878414720452937, 'support': 1038}, 'weighted avg': {'precision': 0.45864343069375374, 'recall': 0.3198458574181118, 'f1-score': 0.3658971810070813, 'support': 1038}}\n",
      "Epoch 2/10, Eval Loss: 0.2802359466751417, Eval Accuracy: 0.9515406698564594\n",
      "Epoch 3/10, Train Loss: 0.0024199778183211324\n",
      "Validation loss per 100 evaluation steps: 0.4457475244998932\n",
      "Validation Loss: 0.2734033852815628\n",
      "Validation Accuracy: 0.9532248803827751\n",
      "F1 Score: 0.4256900212314226\n",
      "{'CHAR': {'precision': 0.4904580152671756, 'recall': 0.31341463414634146, 'f1-score': 0.38244047619047616, 'support': 820}, 'LOC': {'precision': 0.49146757679180886, 'recall': 0.6666666666666666, 'f1-score': 0.5658153241650294, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.4739952718676123, 'recall': 0.38631984585741813, 'f1-score': 0.4256900212314226, 'support': 1038}, 'macro avg': {'precision': 0.32730853068632815, 'recall': 0.3266937669376693, 'f1-score': 0.3160852667851685, 'support': 1038}, 'weighted avg': {'precision': 0.489723091624388, 'recall': 0.38631984585741813, 'f1-score': 0.41986252456246326, 'support': 1038}}\n",
      "Epoch 3/10, Eval Loss: 0.2734033852815628, Eval Accuracy: 0.9532248803827751\n",
      "Epoch 4/10, Train Loss: 0.002069204387374398\n",
      "Validation loss per 100 evaluation steps: 0.4676078259944916\n",
      "Validation Loss: 0.298689836760362\n",
      "Validation Accuracy: 0.9513875598086124\n",
      "F1 Score: 0.40021750951604135\n",
      "{'CHAR': {'precision': 0.4823529411764706, 'recall': 0.3, 'f1-score': 0.3699248120300752, 'support': 820}, 'LOC': {'precision': 0.48412698412698413, 'recall': 0.5648148148148148, 'f1-score': 0.5213675213675214, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.45942571785268416, 'recall': 0.35452793834296725, 'f1-score': 0.40021750951604135, 'support': 1038}, 'macro avg': {'precision': 0.3221599751011516, 'recall': 0.2882716049382716, 'f1-score': 0.29709744446586556, 'support': 1038}, 'weighted avg': {'precision': 0.48179271708683474, 'recall': 0.35452793834296725, 'f1-score': 0.4007261372640138, 'support': 1038}}\n",
      "Epoch 4/10, Eval Loss: 0.298689836760362, Eval Accuracy: 0.9513875598086124\n",
      "Epoch 5/10, Train Loss: 0.002041910441722412\n",
      "Validation loss per 100 evaluation steps: 0.4322371780872345\n",
      "Validation Loss: 0.27841366827487946\n",
      "Validation Accuracy: 0.9559043062200957\n",
      "F1 Score: 0.49313725490196075\n",
      "{'CHAR': {'precision': 0.5176470588235295, 'recall': 0.4292682926829268, 'f1-score': 0.4693333333333333, 'support': 820}, 'LOC': {'precision': 0.5490909090909091, 'recall': 0.6990740740740741, 'f1-score': 0.6150712830957229, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.501996007984032, 'recall': 0.48458574181117536, 'f1-score': 0.49313725490196075, 'support': 1038}, 'macro avg': {'precision': 0.35557932263814623, 'recall': 0.37611412225233365, 'f1-score': 0.36146820547635206, 'support': 1038}, 'weighted avg': {'precision': 0.5231928946039793, 'recall': 0.48458574181117536, 'f1-score': 0.498756002391146, 'support': 1038}}\n",
      "Epoch 5/10, Eval Loss: 0.27841366827487946, Eval Accuracy: 0.9559043062200957\n",
      "Epoch 6/10, Train Loss: 0.0017495214021010371\n",
      "Validation loss per 100 evaluation steps: 0.40728309750556946\n",
      "Validation Loss: 0.31819810916980107\n",
      "Validation Accuracy: 0.9560956937799043\n",
      "F1 Score: 0.3702359346642468\n",
      "{'CHAR': {'precision': 0.5271317829457365, 'recall': 0.24878048780487805, 'f1-score': 0.3380281690140845, 'support': 820}, 'LOC': {'precision': 0.5024630541871922, 'recall': 0.4722222222222222, 'f1-score': 0.48687350835322196, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.4975609756097561, 'recall': 0.2947976878612717, 'f1-score': 0.3702359346642468, 'support': 1038}, 'macro avg': {'precision': 0.34319827904430955, 'recall': 0.24033423667570009, 'f1-score': 0.27496722578910215, 'support': 1038}, 'weighted avg': {'precision': 0.520982737687801, 'recall': 0.2947976878612717, 'f1-score': 0.3683504589555349, 'support': 1038}}\n",
      "Epoch 6/10, Eval Loss: 0.31819810916980107, Eval Accuracy: 0.9560956937799043\n",
      "Epoch 7/10, Train Loss: 0.0021401705646207935\n",
      "Validation loss per 100 evaluation steps: 0.47857192158699036\n",
      "Validation Loss: 0.29408966104189554\n",
      "Validation Accuracy: 0.9550239234449761\n",
      "F1 Score: 0.5116063138347261\n",
      "{'CHAR': {'precision': 0.4916267942583732, 'recall': 0.501219512195122, 'f1-score': 0.4963768115942029, 'support': 820}, 'LOC': {'precision': 0.5185185185185185, 'recall': 0.6481481481481481, 'f1-score': 0.5761316872427984, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.49372759856630827, 'recall': 0.5308285163776493, 'f1-score': 0.5116063138347261, 'support': 1038}, 'macro avg': {'precision': 0.33671510425896384, 'recall': 0.3831225534477567, 'f1-score': 0.3575028329456671, 'support': 1038}, 'weighted avg': {'precision': 0.4962755022079634, 'recall': 0.5308285163776493, 'f1-score': 0.5120167918609738, 'support': 1038}}\n",
      "Epoch 7/10, Eval Loss: 0.29408966104189554, Eval Accuracy: 0.9550239234449761\n",
      "Epoch 8/10, Train Loss: 0.001453282471332813\n",
      "Validation loss per 100 evaluation steps: 0.4511321783065796\n",
      "Validation Loss: 0.293552575343185\n",
      "Validation Accuracy: 0.9552918660287081\n",
      "F1 Score: 0.41298265249020705\n",
      "{'CHAR': {'precision': 0.4722753346080306, 'recall': 0.30121951219512194, 'f1-score': 0.36783320923306023, 'support': 820}, 'LOC': {'precision': 0.583732057416268, 'recall': 0.5648148148148148, 'f1-score': 0.5741176470588235, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.49265687583444595, 'recall': 0.3554913294797688, 'f1-score': 0.41298265249020705, 'support': 1038}, 'macro avg': {'precision': 0.3520024640080995, 'recall': 0.2886781090033122, 'f1-score': 0.31398361876396125, 'support': 1038}, 'weighted avg': {'precision': 0.4945586693453748, 'recall': 0.3554913294797688, 'f1-score': 0.4100507161231361, 'support': 1038}}\n",
      "Epoch 8/10, Eval Loss: 0.293552575343185, Eval Accuracy: 0.9552918660287081\n",
      "Epoch 9/10, Train Loss: 0.001180498538184222\n",
      "Validation loss per 100 evaluation steps: 0.459012508392334\n",
      "Validation Loss: 0.33160408039887745\n",
      "Validation Accuracy: 0.9546411483253588\n",
      "F1 Score: 0.34469696969696967\n",
      "{'CHAR': {'precision': 0.48135593220338985, 'recall': 0.17317073170731706, 'f1-score': 0.2547085201793722, 'support': 820}, 'LOC': {'precision': 0.5504201680672269, 'recall': 0.6064814814814815, 'f1-score': 0.5770925110132158, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.5, 'recall': 0.2630057803468208, 'f1-score': 0.34469696969696967, 'support': 1038}, 'macro avg': {'precision': 0.34392536675687224, 'recall': 0.2598840710629329, 'f1-score': 0.2772670103975293, 'support': 1038}, 'weighted avg': {'precision': 0.4948002126293841, 'recall': 0.2630057803468208, 'f1-score': 0.3213034382716183, 'support': 1038}}\n",
      "Epoch 9/10, Eval Loss: 0.33160408039887745, Eval Accuracy: 0.9546411483253588\n",
      "Epoch 10/10, Train Loss: 0.0009276909894076513\n",
      "Validation loss per 100 evaluation steps: 0.5322247743606567\n",
      "Validation Loss: 0.3161833932002385\n",
      "Validation Accuracy: 0.954755980861244\n",
      "F1 Score: 0.46337662337662344\n",
      "{'CHAR': {'precision': 0.5015974440894568, 'recall': 0.3829268292682927, 'f1-score': 0.4343015214384509, 'support': 820}, 'LOC': {'precision': 0.5714285714285714, 'recall': 0.6111111111111112, 'f1-score': 0.5906040268456376, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.5028184892897407, 'recall': 0.4296724470134875, 'f1-score': 0.46337662337662344, 'support': 1038}, 'macro avg': {'precision': 0.3576753385060094, 'recall': 0.33134598012646793, 'f1-score': 0.34163518276136284, 'support': 1038}, 'weighted avg': {'precision': 0.5151623078823949, 'recall': 0.4296724470134875, 'f1-score': 0.4659900938132827, 'support': 1038}}\n",
      "Epoch 10/10, Eval Loss: 0.3161833932002385, Eval Accuracy: 0.954755980861244\n",
      "   epoch  train_loss  eval_loss  accuracy  f1_score\n",
      "0      1    0.003666   0.270379  0.953569  0.371243\n",
      "1      2    0.002797   0.280236  0.951541  0.375566\n",
      "2      3    0.002420   0.273403  0.953225  0.425690\n",
      "3      4    0.002069   0.298690  0.951388  0.400218\n",
      "4      5    0.002042   0.278414  0.955904  0.493137\n",
      "5      6    0.001750   0.318198  0.956096  0.370236\n",
      "6      7    0.002140   0.294090  0.955024  0.511606\n",
      "7      8    0.001453   0.293553  0.955292  0.412983\n",
      "8      9    0.001180   0.331604  0.954641  0.344697\n",
      "9     10    0.000928   0.316183  0.954756  0.463377\n",
      "    epoch         label  precision    recall  f1-score  support\n",
      "0       1          CHAR   0.482759  0.239024  0.319739      820\n",
      "1       1           LOC   0.502110  0.550926  0.525386      216\n",
      "2       1           ORG   0.000000  0.000000  0.000000        2\n",
      "3       1     micro avg   0.477997  0.303468  0.371243     1038\n",
      "4       1     macro avg   0.328289  0.263317  0.281708     1038\n",
      "5       1  weighted avg   0.485855  0.303468  0.361917     1038\n",
      "6       2          CHAR   0.448352  0.248780  0.320000      820\n",
      "7       2           LOC   0.501961  0.592593  0.543524      216\n",
      "8       2           ORG   0.000000  0.000000  0.000000        2\n",
      "9       2     micro avg   0.454795  0.319846  0.375566     1038\n",
      "10      2     macro avg   0.316771  0.280458  0.287841     1038\n",
      "11      2  weighted avg   0.458643  0.319846  0.365897     1038\n",
      "12      3          CHAR   0.490458  0.313415  0.382440      820\n",
      "13      3           LOC   0.491468  0.666667  0.565815      216\n",
      "14      3           ORG   0.000000  0.000000  0.000000        2\n",
      "15      3     micro avg   0.473995  0.386320  0.425690     1038\n",
      "16      3     macro avg   0.327309  0.326694  0.316085     1038\n",
      "17      3  weighted avg   0.489723  0.386320  0.419863     1038\n",
      "18      4          CHAR   0.482353  0.300000  0.369925      820\n",
      "19      4           LOC   0.484127  0.564815  0.521368      216\n",
      "20      4           ORG   0.000000  0.000000  0.000000        2\n",
      "21      4     micro avg   0.459426  0.354528  0.400218     1038\n",
      "22      4     macro avg   0.322160  0.288272  0.297097     1038\n",
      "23      4  weighted avg   0.481793  0.354528  0.400726     1038\n",
      "24      5          CHAR   0.517647  0.429268  0.469333      820\n",
      "25      5           LOC   0.549091  0.699074  0.615071      216\n",
      "26      5           ORG   0.000000  0.000000  0.000000        2\n",
      "27      5     micro avg   0.501996  0.484586  0.493137     1038\n",
      "28      5     macro avg   0.355579  0.376114  0.361468     1038\n",
      "29      5  weighted avg   0.523193  0.484586  0.498756     1038\n",
      "30      6          CHAR   0.527132  0.248780  0.338028      820\n",
      "31      6           LOC   0.502463  0.472222  0.486874      216\n",
      "32      6           ORG   0.000000  0.000000  0.000000        2\n",
      "33      6     micro avg   0.497561  0.294798  0.370236     1038\n",
      "34      6     macro avg   0.343198  0.240334  0.274967     1038\n",
      "35      6  weighted avg   0.520983  0.294798  0.368350     1038\n",
      "36      7          CHAR   0.491627  0.501220  0.496377      820\n",
      "37      7           LOC   0.518519  0.648148  0.576132      216\n",
      "38      7           ORG   0.000000  0.000000  0.000000        2\n",
      "39      7     micro avg   0.493728  0.530829  0.511606     1038\n",
      "40      7     macro avg   0.336715  0.383123  0.357503     1038\n",
      "41      7  weighted avg   0.496276  0.530829  0.512017     1038\n",
      "42      8          CHAR   0.472275  0.301220  0.367833      820\n",
      "43      8           LOC   0.583732  0.564815  0.574118      216\n",
      "44      8           ORG   0.000000  0.000000  0.000000        2\n",
      "45      8     micro avg   0.492657  0.355491  0.412983     1038\n",
      "46      8     macro avg   0.352002  0.288678  0.313984     1038\n",
      "47      8  weighted avg   0.494559  0.355491  0.410051     1038\n",
      "48      9          CHAR   0.481356  0.173171  0.254709      820\n",
      "49      9           LOC   0.550420  0.606481  0.577093      216\n",
      "50      9           ORG   0.000000  0.000000  0.000000        2\n",
      "51      9     micro avg   0.500000  0.263006  0.344697     1038\n",
      "52      9     macro avg   0.343925  0.259884  0.277267     1038\n",
      "53      9  weighted avg   0.494800  0.263006  0.321303     1038\n",
      "54     10          CHAR   0.501597  0.382927  0.434302      820\n",
      "55     10           LOC   0.571429  0.611111  0.590604      216\n",
      "56     10           ORG   0.000000  0.000000  0.000000        2\n",
      "57     10     micro avg   0.502818  0.429672  0.463377     1038\n",
      "58     10     macro avg   0.357675  0.331346  0.341635     1038\n",
      "59     10  weighted avg   0.515162  0.429672  0.465990     1038\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score as seq_f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Assuming replace_per_tags, read_data, and dataset are defined as in previous steps\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1 = seq_f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1, report\n",
    "\n",
    "\n",
    "# Lists to store metrics for each epoch\n",
    "all_metrics = []\n",
    "all_reports = []\n",
    "\n",
    "# Training and evaluation for each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "    \n",
    "    eval_loss, eval_accuracy, F1, eval_report = valid(model, testing_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "    \n",
    "    # Store the metrics for this epoch\n",
    "    metrics = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"accuracy\": eval_accuracy,\n",
    "        \"f1_score\": F1\n",
    "    }\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    # Flatten the classification report for this epoch\n",
    "    flat_report = []\n",
    "    for label, scores in eval_report.items():\n",
    "        flat_report.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"label\": label,\n",
    "            \"precision\": scores[\"precision\"],\n",
    "            \"recall\": scores[\"recall\"],\n",
    "            \"f1-score\": scores[\"f1-score\"],\n",
    "            \"support\": scores[\"support\"]\n",
    "        })\n",
    "    all_reports.extend(flat_report)\n",
    "\n",
    "# Convert the lists of metrics and reports to DataFrames\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "reports_df = pd.DataFrame(all_reports)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(metrics_df)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "634bc4b5-cc0f-43f8-9f94-6fea573c7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/EWT_epochs_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00e7df08-4982-4059-b332-9f2b7342fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.to_csv('data/EWT_epochs_reports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdf607-39af-487a-8de9-d951762f4129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30d36d-54be-43ab-8458-3d3488df4f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d7aaf3-0633-4f0a-b4fd-8fccda09385d",
   "metadata": {},
   "source": [
    "Train EWT+LOTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8c5d4-996f-4cd9-9f17-85f9d9af089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Memory Allocated: 4.13 GB\n",
      "  Memory Cached: 11.97 GB\n",
      "O: 25000\n",
      "B-CHAR: 820\n",
      "I-CHAR: 85\n",
      "B-LOC: 216\n",
      "B-ORG: 2\n",
      "I-LOC: 2\n",
      "Initial tag counts in test_tags: {'O': 25000, 'B-CHAR': 820, 'I-CHAR': 85, 'B-LOC': 216, 'B-ORG': 2, 'I-LOC': 2}\n",
      "Epoch 1/7, Train Loss: 0.0011966206406399417\n",
      "Epoch 2/7, Train Loss: 0.0009113156999200716\n",
      "Epoch 3/7, Train Loss: 0.000998297705215229\n",
      "Epoch 4/7, Train Loss: 0.0005944750726566028\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "# Data Reading and Preprocessing Functions\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "\n",
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in test_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts = dict(tag_counts)\n",
    "\n",
    "# Print the counts for each tag\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "labels_to_ids = {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
    "ids_to_labels = {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
    "\n",
    "# Create training and testing datasets\n",
    "training_set = dataset(df, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "beeee824-0a8d-40db-943b-01257fdf0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/EWT_LOTR_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f381240d-46b7-4db4-afdb-970d5d9218d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.to_csv('data/EWT_LOTR_reports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eee338-1680-47d0-95cf-9e28ead1eddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fc75edf5-3cc9-4849-ad08-90ab01c80606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"EWT-LOTR-baseline\")\n",
    "tokenizer.save_pretrained('EWT-LOTR-tokenizer')\n",
    "import json\n",
    "config = json.load(open('EWT-LOTR-baseline/config.json'))\n",
    "config['id2label'] = ids_to_labels\n",
    "config['label2id'] = labels_to_ids\n",
    "json.dump(config, open('EWT-LOTR-baseline/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bbda5-0ebe-494b-bc81-cbeaca41fc11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-try)",
   "language": "python",
   "name": "bert-try"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
