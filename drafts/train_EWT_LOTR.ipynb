{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5eb96d-fd04-4697-a73d-2251e4621bdc",
   "metadata": {},
   "source": [
    "Training just the EWT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2611f1ed-604c-4f4f-872f-e3c227b85fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Memory Allocated: 0.00 GB\n",
      "  Memory Cached: 0.00 GB\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "# Data Reading and Preprocessing Functions\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "    \n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./en_ewt-ud-train.iob2\")\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "def replace_per_tags(tags_list):\n",
    "    updated_tags_list = []\n",
    "    for tags in tags_list:\n",
    "        updated_tags = []\n",
    "        for tag in tags:\n",
    "            if tag == \"B-PER\":\n",
    "                updated_tags.append(\"B-CHAR\")\n",
    "            elif tag == \"I-PER\":\n",
    "                updated_tags.append(\"I-CHAR\")\n",
    "            else:\n",
    "                updated_tags.append(tag)\n",
    "        updated_tags_list.append(updated_tags)\n",
    "    return updated_tags_list\n",
    "\n",
    "train_tags = replace_per_tags(train_tags)\n",
    "\n",
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4cec580-a99f-4d9c-ad24-de07081d04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a1ad31-c0d3-44a8-8d3c-8fdb44768785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_counts_train: \n",
      "O: 194219\n",
      "B-LOC: 2712\n",
      "I-LOC: 877\n",
      "B-CHAR: 2874\n",
      "B-ORG: 1436\n",
      "I-ORG: 1167\n",
      "I-CHAR: 1294\n",
      "labels_to_ids: {'I-LOC': 0, 'B-CHAR': 1, 'O': 2, 'B-LOC': 3, 'I-CHAR': 4, 'I-ORG': 5, 'B-ORG': 6}\n",
      "ids_to_labels: {0: 'I-LOC', 1: 'B-CHAR', 2: 'O', 3: 'B-LOC', 4: 'I-CHAR', 5: 'I-ORG', 6: 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts_train = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in train_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts_train[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts_train = dict(tag_counts_train)\n",
    "\n",
    "# Print the counts for each tag\n",
    "print('tag_counts_train: ')\n",
    "for tag, count in tag_counts_train.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "# Create mappings\n",
    "all_tags = [tag for tags in df['word_labels'] for tag in tags.split(\",\")]\n",
    "unique_tags = set(all_tags)\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_tags)}\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "\n",
    "# Display the mappings\n",
    "print(\"labels_to_ids:\", labels_to_ids)\n",
    "print(\"ids_to_labels:\", ids_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62797808-8374-4af6-8a5f-180404114fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial tag counts in train_tags: {'O': 194219, 'B-LOC': 2712, 'I-LOC': 877, 'B-CHAR': 2874, 'B-ORG': 1436, 'I-ORG': 1167, 'I-CHAR': 1294}\n",
      "Initial tag counts in test_tags: {'O': 25000, 'B-CHAR': 820, 'I-CHAR': 85, 'B-LOC': 216, 'B-ORG': 2, 'I-LOC': 2}\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing datasets\n",
    "training_set = dataset(df, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "initial_train_tag_counts = count_tags(train_tags)\n",
    "print(\"Initial tag counts in train_tags:\", dict(initial_train_tag_counts))\n",
    "\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14acec1-81a0-40c6-8b86-4e24e05892f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7, Train Loss: 0.13014855739489503\n",
      "Epoch 2/7, Train Loss: 0.02903342734290553\n",
      "Epoch 3/7, Train Loss: 0.015176992154233538\n",
      "Epoch 4/7, Train Loss: 0.009204415571804596\n",
      "Epoch 5/7, Train Loss: 0.005340539203476807\n",
      "Epoch 6/7, Train Loss: 0.004014212377987118\n",
      "Epoch 7/7, Train Loss: 0.003184488379664966\n",
      "Validation loss per 100 evaluation steps: 0.43129369616508484\n",
      "Validation Loss: 0.24635170499483744\n",
      "Validation Accuracy: 0.9566698564593301\n",
      "F1 Score: 0.5136071251855517\n",
      "{'CHAR': {'precision': 0.5512820512820513, 'recall': 0.4195121951219512, 'f1-score': 0.47645429362880887, 'support': 820}, 'LOC': {'precision': 0.546875, 'recall': 0.8101851851851852, 'f1-score': 0.6529850746268657, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.5279755849440488, 'recall': 0.5, 'f1-score': 0.5136071251855517, 'support': 1038}, 'macro avg': {'precision': 0.36605235042735046, 'recall': 0.4098991267690455, 'f1-score': 0.37647978941855814, 'support': 1038}, 'weighted avg': {'precision': 0.549302776542661, 'recall': 0.5, 'f1-score': 0.5122709989354781, 'support': 1038}}\n",
      "Eval Loss: 0.24635170499483744, Eval Accuracy: 0.9566698564593301\n",
      "{'CHAR': {'precision': 0.5512820512820513, 'recall': 0.4195121951219512, 'f1-score': 0.47645429362880887, 'support': 820}, 'LOC': {'precision': 0.546875, 'recall': 0.8101851851851852, 'f1-score': 0.6529850746268657, 'support': 216}, 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'micro avg': {'precision': 0.5279755849440488, 'recall': 0.5, 'f1-score': 0.5136071251855517, 'support': 1038}, 'macro avg': {'precision': 0.36605235042735046, 'recall': 0.4098991267690455, 'f1-score': 0.37647978941855814, 'support': 1038}, 'weighted avg': {'precision': 0.549302776542661, 'recall': 0.5, 'f1-score': 0.5122709989354781, 'support': 1038}}\n",
      "   eval_loss  accuracy  f1_score  \\\n",
      "0   0.246352   0.95667  0.513607   \n",
      "\n",
      "                                              report  \n",
      "0  {'CHAR': {'precision': 0.5512820512820513, 're...  \n",
      "          label  precision    recall  f1-score  support\n",
      "0          CHAR   0.551282  0.419512  0.476454      820\n",
      "1           LOC   0.546875  0.810185  0.652985      216\n",
      "2           ORG   0.000000  0.000000  0.000000        2\n",
      "3     micro avg   0.527976  0.500000  0.513607     1038\n",
      "4     macro avg   0.366052  0.409899  0.376480     1038\n",
      "5  weighted avg   0.549303  0.500000  0.512271     1038\n"
     ]
    }
   ],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels_to_ids))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c249e27-5567-4497-a2ee-00cb4775ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/EWT-training_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f976551-4451-49ac-b118-f86587b80cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df.to_csv('data/EWT-training-report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a51ada9-42d3-43a5-ba0f-964a9d6133c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EWT-tokenizer/tokenizer_config.json',\n",
       " 'EWT-tokenizer/special_tokens_map.json',\n",
       " 'EWT-tokenizer/vocab.txt',\n",
       " 'EWT-tokenizer/added_tokens.json',\n",
       " 'EWT-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"EWT-baseline\")\n",
    "tokenizer.save_pretrained('EWT-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e014a36b-d000-4b3f-909c-379ef392f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.load(open('EWT-baseline/config.json'))\n",
    "config['id2label'] = ids_to_labels\n",
    "config['label2id'] = labels_to_ids\n",
    "json.dump(config, open('EWT-baseline/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233c5ca0-45dc-4723-82b6-453b2b26c713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "  Memory Allocated: 1.65 GB\n",
      "  Memory Cached: 9.74 GB\n",
      "O: 25000\n",
      "B-CHAR: 820\n",
      "I-CHAR: 85\n",
      "B-LOC: 216\n",
      "B-ORG: 2\n",
      "I-LOC: 2\n",
      "Initial tag counts in test_tags: {'O': 25000, 'B-CHAR': 820, 'I-CHAR': 85, 'B-LOC': 216, 'B-ORG': 2, 'I-LOC': 2}\n",
      "Epoch 1/7, Train Loss: 0.1443906251094927\n",
      "Epoch 2/7, Train Loss: 0.00830291956477757\n",
      "Epoch 3/7, Train Loss: 0.004374322391824744\n",
      "Epoch 4/7, Train Loss: 0.0026943455397405005\n",
      "Epoch 5/7, Train Loss: 0.0019048888936629934\n",
      "Epoch 6/7, Train Loss: 0.0012014668715144893\n",
      "Epoch 7/7, Train Loss: 0.0011562481729716365\n",
      "Validation loss per 100 evaluation steps: 0.1820465475320816\n",
      "Validation Loss: 0.06642799030782448\n",
      "Validation Accuracy: 0.9905454545454545\n",
      "F1 Score: 0.8769898697539797\n",
      "{'CHAR': {'precision': 0.8631319358816276, 'recall': 0.8536585365853658, 'f1-score': 0.8583690987124464, 'support': 820}, 'LOC': {'precision': 0.9327354260089686, 'recall': 0.9629629629629629, 'f1-score': 0.9476082004555809, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.8782608695652174, 'recall': 0.8757225433526011, 'f1-score': 0.8769898697539797, 'support': 1038}, 'macro avg': {'precision': 0.9319557872968653, 'recall': 0.7722071665161095, 'f1-score': 0.8242146552782312, 'support': 1038}, 'weighted avg': {'precision': 0.8778796141048861, 'recall': 0.8757225433526011, 'f1-score': 0.8765697163544749, 'support': 1038}}\n",
      "Eval Loss: 0.06642799030782448, Eval Accuracy: 0.9905454545454545\n",
      "{'CHAR': {'precision': 0.8631319358816276, 'recall': 0.8536585365853658, 'f1-score': 0.8583690987124464, 'support': 820}, 'LOC': {'precision': 0.9327354260089686, 'recall': 0.9629629629629629, 'f1-score': 0.9476082004555809, 'support': 216}, 'ORG': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'micro avg': {'precision': 0.8782608695652174, 'recall': 0.8757225433526011, 'f1-score': 0.8769898697539797, 'support': 1038}, 'macro avg': {'precision': 0.9319557872968653, 'recall': 0.7722071665161095, 'f1-score': 0.8242146552782312, 'support': 1038}, 'weighted avg': {'precision': 0.8778796141048861, 'recall': 0.8757225433526011, 'f1-score': 0.8765697163544749, 'support': 1038}}\n",
      "   eval_loss  accuracy  f1_score  \\\n",
      "0   0.066428  0.990545   0.87699   \n",
      "\n",
      "                                              report  \n",
      "0  {'CHAR': {'precision': 0.8631319358816276, 're...  \n",
      "          label  precision    recall  f1-score  support\n",
      "0          CHAR   0.863132  0.853659  0.858369      820\n",
      "1           LOC   0.932735  0.962963  0.947608      216\n",
      "2           ORG   1.000000  0.500000  0.666667        2\n",
      "3     micro avg   0.878261  0.875723  0.876990     1038\n",
      "4     macro avg   0.931956  0.772207  0.824215     1038\n",
      "5  weighted avg   0.877880  0.875723  0.876570     1038\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report    \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "MAX_LEN = 174\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 7\n",
    "MAX_GRAD_NORM = 5\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "# Data Reading and Preprocessing Functions\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    \n",
    "    # Get the name and other details of each GPU\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.sentence[index].strip().split()\n",
    "        word_labels = self.data.word_labels[index].split(\",\")\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len)\n",
    "\n",
    "        labels = [labels_to_ids[label] for label in word_labels]\n",
    "\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def read_data(file_path):\n",
    "    sentences, labels = [], []\n",
    "    sentence, label = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            elif line == \"\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "            else:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                sentence.append(parts[1].lower())  # Convert the token to lowercase before appending\n",
    "                label.append(clean_tag(parts[2]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "def clean_tag(tag):\n",
    "    if tag.count('-') > 1:\n",
    "        prefix, entity = tag.split('-', 1)\n",
    "        tag = f\"{prefix}-{entity.replace('-', '')}\"\n",
    "    return tag\n",
    "\n",
    "def train_model(training_set, model, optimizer):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        tr_logits = outputs.logits\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    return epoch_loss\n",
    "\n",
    "train_tokens, train_tags = read_data(\"./tagged_sentences_train.iob2\")\n",
    "test_tokens, test_tags = read_data(\"./tagged_sentences_test.iob2\")\n",
    "\n",
    "data = {'sentence': [\" \".join(sentence) for sentence in train_tokens],\n",
    "        'word_labels': [\",\".join(tags) for tags in train_tags]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "data_test = {'sentence': [\" \".join(sentence) for sentence in test_tokens],\n",
    "             'word_labels': [\",\".join(tags) for tags in test_tags]}\n",
    "\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "# Iterate through each list in test_tags and count the occurrences of each tag\n",
    "for sentence in test_tags:\n",
    "    for tag in sentence:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Convert the defaultdict to a regular dictionary for easier printing\n",
    "tag_counts = dict(tag_counts)\n",
    "\n",
    "# Print the counts for each tag\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "labels_to_ids = {'B-CHAR': 0, 'O': 1, 'I-CHAR': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n",
    "ids_to_labels = {0: 'B-CHAR', 1: 'O', 2: 'I-CHAR', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
    "\n",
    "# Create training and testing datasets\n",
    "training_set = dataset(df, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(df_test, tokenizer, MAX_LEN)\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE, 'shuffle': False, 'num_workers': 0}\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "# Function to count tag occurrences\n",
    "def count_tags(tags_list):\n",
    "    tag_counts = defaultdict(int)\n",
    "    for sentence in tags_list:\n",
    "        for tag in sentence:\n",
    "            tag_counts[tag] += 1\n",
    "    return tag_counts\n",
    "\n",
    "# Count initial tag occurrences in test_tags\n",
    "initial_tag_counts = count_tags(test_tags)\n",
    "print(\"Initial tag counts in test_tags:\", dict(initial_tag_counts))\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_logits = outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                loss_step = eval_loss / nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # Compute evaluation accuracy\n",
    "            active_logits = eval_logits.view(-1, model.config.num_labels)  # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)  # shape (batch_size * seq_len,)\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = flattened_predictions.view(labels.size(0), labels.size(1))[i]\n",
    "\n",
    "                active_accuracy = label != -100  # shape (seq_len,)\n",
    "                label = torch.masked_select(label, active_accuracy)\n",
    "                pred = torch.masked_select(pred, active_accuracy)\n",
    "\n",
    "                eval_labels.append([ids_to_labels[id.item()] for id in label])\n",
    "                eval_preds.append([ids_to_labels[id.item()] for id in pred])\n",
    "\n",
    "                tmp_eval_accuracy = accuracy_score(label.cpu().numpy(), pred.cpu().numpy())\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    F1_score = f1_score(eval_labels, eval_preds)\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"F1 Score: {F1_score}\")\n",
    "    report = seqeval_classification_report(eval_labels, eval_preds, output_dict=True)\n",
    "    print(report)\n",
    "    \n",
    "    return eval_loss, eval_accuracy, F1_score, report\n",
    "\n",
    "\n",
    "# Train and evaluate the model on the entire dataset\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_model(training_set, model, optimizer)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss}\")\n",
    "\n",
    "# Evaluating the model\n",
    "eval_loss, eval_accuracy, f1_score, eval_report = valid(model, testing_loader)\n",
    "print(f\"Eval Loss: {eval_loss}, Eval Accuracy: {eval_accuracy}\")\n",
    "print(eval_report)\n",
    "\n",
    "\n",
    "# Display the evaluation metrics in a DataFrame\n",
    "metrics = {\n",
    "    \"eval_loss\": eval_loss,\n",
    "    \"accuracy\": eval_accuracy,\n",
    "    \"f1_score\": f1_score,\n",
    "    \"report\": eval_report\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "print(metrics_df)\n",
    "\n",
    "# Flatten the classification report for easier viewing\n",
    "flat_reports = []\n",
    "for label, scores in eval_report.items():\n",
    "    flat_reports.append({\n",
    "        \"label\": label,\n",
    "        \"precision\": scores[\"precision\"],\n",
    "        \"recall\": scores[\"recall\"],\n",
    "        \"f1-score\": scores[\"f1-score\"],\n",
    "        \"support\": scores[\"support\"]\n",
    "    })\n",
    "\n",
    "reports_df = pd.DataFrame(flat_reports)\n",
    "print(reports_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a574c-b2db-47be-beba-cc6416bbf681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595eb4f-5ecd-4686-ac71-095df608aec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cfba2-a973-45fb-96cc-9c2ced42cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv('data/EWT_LOTR_metrics.csv', index=False)\n",
    "reports_df.to_csv('data/EWT_LOTR_reports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9520ae-a7e5-4f5f-b8f1-d4f241c4019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"EWT-LOTR-baseline\")\n",
    "tokenizer.save_pretrained('EWT-LOTR-tokenizer')\n",
    "import json\n",
    "config = json.load(open('EWT-LOTR-baseline/config.json'))\n",
    "config['id2label'] = ids_to_labels\n",
    "config['label2id'] = labels_to_ids\n",
    "json.dump(config, open('EWT-LOTR-baseline/config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dc5fd-603c-46c8-afdb-69f9ab4591e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f8312-e5d9-4e69-b4a0-89786f5e1d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cbe3a-a407-4d58-9389-267892036ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03266b-51eb-4943-be34-8b106cb9ead7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75750a02-a96c-41f3-8aa5-7fdc82f2d481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53666b5d-0f1c-4121-b04b-891adf6b31ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924b241-c179-4ad1-9ead-2cc4a3db0785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-try)",
   "language": "python",
   "name": "bert-try"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
