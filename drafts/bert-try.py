# -*- coding: utf-8 -*-
"""bert_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EQnszhzVA4Vfo79pbDAu0Er_KePFb07J
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast, BertForTokenClassification, AdamW
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

## Configuration
MAX_LEN = 174
BATCH_SIZE = 64
EPOCHS = 1
MODEL_NAME = 'bert-base-uncased'

## Reading the data
def read_data(file_path):
    file = open(file_path, encoding="utf-8")
    sentences = []
    labels = []
    sentence = []
    label = []

    for line in file:
        if line.startswith("#"):
            continue
        elif line == "\n":
            if sentence:
                sentences.append(sentence)
                labels.append(label)
                sentence = []
                label = []
        else:
            parts = line.strip().split("\t")
            sentence.append(parts[1].lower())  # Convert the token to lowercase before appending
            label.append(parts[2])

    if sentence:
        sentences.append(sentence)
        labels.append(label)

    file.close()
    return sentences, labels

def tokenize_and_preserve_labels(token_docs, tag_docs, tokenizer):
    tokenized_texts = []
    labels = []

    for word_list, label_list in zip(token_docs, tag_docs):
        tmp_tokens = []
        tmp_labels = []

        for word, label in zip(word_list, label_list):
            tokenized_word = tokenizer.tokenize(word)
            n_subwords = len(tokenized_word)

            tmp_tokens.extend(tokenized_word)
            tmp_labels.extend([label] * n_subwords)

        tokenized_texts.append(tmp_tokens)
        labels.append(tmp_labels)

    return tokenized_texts, labels

class NERDataset(Dataset):
    def __init__(self, sentences, tags, tokenizer, max_len, tag2idx):
        self.sentences = sentences
        self.tags = tags
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.tag2idx = tag2idx

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        word_labels = self.tags[idx]

        # Tokenize the sentence and map labels to tokenized words
        encoding = self.tokenizer(sentence,
                                  is_split_into_words=True,
                                  return_offsets_mapping=True,
                                  padding='max_length',
                                  truncation=True,
                                  max_length=self.max_len,
                                  return_tensors='pt')

        labels = [self.tag2idx['O']] * self.max_len  # Initialize labels with "O"
        offsets = encoding['offset_mapping'].squeeze().tolist()  # Get the offsets
        encoding.pop('offset_mapping')  # Remove offsets, not needed for model input

        # Assign label to the first token of each word, subsequent subtokens get label -100 (ignored)
        idx = 0
        for i, (start, end) in enumerate(offsets):
            if start == end:  # Special tokens
                labels[i] = self.tag2idx['O']
            elif start == 0:  # Start of a new word
                labels[i] = self.tag2idx[word_labels[idx]]
                idx += 1
            else:  # Subtoken of a word
                labels[i] = -100  # PyTorch's convention to ignore these tokens in loss computation

        item = {key: val.squeeze() for key, val in encoding.items()}  # Remove batch dimension
        item['labels'] = torch.tensor(labels)

        return item

def read_names(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        names = [name.strip().lower() for name in file.readlines()]
    return names

character_names = read_names('./scraping_res/character_names.txt')
location_names = read_names('./scraping_res/location_names.txt')
organization_names = read_names('./scraping_res/organization_names.txt')

all_names = character_names + location_names + organization_names

tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)
num_added_toks = tokenizer.add_tokens(all_names)

train_tokens, train_tags = read_data("./tagged_sentences_train.iob2")
tag_values = list(set(tag for doc in train_tags for tag in doc))
tag_values.append("PAD")
tag2idx = {tag: idx for idx, tag in enumerate(tag_values)}
idx2tag = dict([(value, key) for key, value in tag2idx.items()])

class NERDataset_Skip(Dataset):
    def __init__(self, sentences, tags, tokenizer, max_len, tag2idx):
        self.sentences = sentences
        self.tags = tags
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.tag2idx = tag2idx

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        word_labels = self.tags[idx]

        # Skip tokenization and directly use the original sentence
        input_ids = [self.tokenizer.vocab.get(token, self.tokenizer.vocab['[UNK]']) for token in sentence]
        input_ids = input_ids[:self.max_len] + [0] * (self.max_len - len(input_ids))  # Padding to max_len

        labels = [self.tag2idx[label] for label in word_labels]
        labels = labels[:self.max_len] + [self.tag2idx['O']] * (self.max_len - len(labels))  # Padding to max_len

        input_ids = torch.tensor(input_ids)
        labels = torch.tensor(labels)

        return {'input_ids': input_ids, 'attention_mask': (input_ids != 0).long(), 'labels': labels}

def train_and_evaluate(dataset_class, tokenizer, tag2idx, idx2tag, train_tokens, train_tags, size):
    # Use a subset of the training data
    train_tokens_subset, _, train_tags_subset, _ = train_test_split(train_tokens, train_tags, train_size=size, random_state=42)

    # Prepare the dataset and dataloader
    train_data = dataset_class(train_tokens_subset, train_tags_subset, tokenizer, MAX_LEN, tag2idx)
    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)

    # Initialize model
    model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(tag2idx))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    optimizer = AdamW(model.parameters(), lr=3e-5)

    # Training loop
    model.train()
    for epoch in range(EPOCHS):
        for step, batch in enumerate(train_loader):
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if step % 10 == 0:
                print(f"Epoch {epoch + 1}, Step {step}, Loss: {loss.item()}")

    # Evaluate the model
    test_sentences, test_labels = read_data('tagged_sentences_test.iob2')
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for sentence, label in zip(test_sentences, test_labels):
            inputs = tokenizer(sentence, return_tensors="pt", padding='max_length', truncation=True, max_length=MAX_LEN)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            outputs = model(**inputs)
            loss = outputs.loss
            total_loss += loss.item()

    average_loss = total_loss / len(test_sentences)
    return average_loss

def generate_learning_curve(dataset_class, tokenizer, tag2idx, idx2tag):
    train_tokens, train_tags = read_data("./tagged_sentences_train.iob2")
    sizes = [0.1, 0.25, 0.5, 0.75, 1.0]  # Different percentages of the data to use
    losses = []

    for size in sizes:
        print(f"Training with {int(size*100)}% of the data")
        loss = train_and_evaluate(dataset_class, tokenizer, tag2idx, idx2tag, train_tokens, train_tags, size)
        losses.append(loss)

    plt.figure(figsize=(10, 6))
    plt.plot([size*100 for size in sizes], losses, marker='o')
    plt.title('Learning Curve')
    plt.xlabel('Training Data Size (%)')
    plt.ylabel('Average Loss')
    plt.grid(True)
    plt.show()

# Generate the learning curve
generate_learning_curve(NERDataset, tokenizer, tag2idx, idx2tag)
